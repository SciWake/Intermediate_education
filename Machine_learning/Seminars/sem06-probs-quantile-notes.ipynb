{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d22b1225",
   "metadata": {},
   "source": [
    "<h1 style=\"color:black\" align=\"center\">Семинар №6</h1>\n",
    "\n",
    "<h1 style=\"color:#008B8B\">1. Предсказание вероятностей</h1>\n",
    "\n",
    "Примеры, где необходимо оценивать вероятность:\n",
    "\n",
    "**Пример 1.** Имеется модель $b(x)$, которая решает задачу бинарной классификации предсказания $\\mathbb{Y} = \\{-1, +1\\}$. Где $b(x) \\in [0, 1]$. Если взять все объекты где $b(x) \\approx p$ примерно, так как у нас конечная выборка и не будет много объектов где один и тот же выход модели, который отличается +-0.005 и среди этих объектов будет примерно доля $p$ положительных.\n",
    "\n",
    "Тогда мы можем делать выводы того, что модель уверена в своем прогнозе. А что значит если модель уверена? Ну если модель удовлетворяет определению выше, значит она оценивает вероятность и если модель уверена, значит $b(x)$ буде близко к 0 или 1, а неуверена модель будет когда $b(x) \\approx \\frac{1}{2}$.\n",
    "\n",
    "**Пример 2.** Расчет математического ожидания.\n",
    "\n",
    "Формальное требование оценивания вероятностей записывается:\n",
    "\n",
    "$$\\large \\underset{b \\in \\mathbb{R}}{\\text{argmin}} =\\mathbb{E} (L(y, b)|x) = p(y = +1 | x)$$\n",
    "\n",
    "Значит, функция потреь должна устроена так, чтобы в конкретной точке $x$ для модели было оптимально выдавать вероятность положительного класса. Но, наша модель параметрическая, это некоторая функция, которая рассчитывает прогноз в некоторой точке, поэтому мы не сможем в каждой точке выдавать вероятность, так как модель параметризованная (ограниченная) и она не способна восстановить все функции, но модель будет стремиться выдавать вероятности в каждой точке.\n",
    "\n",
    "И данное требование выполнялось для следующих функций потерь:\n",
    "\n",
    "$\\large L(y, b) = (b - [y = +1])^2$ - Запишем целевую переменную через индикатор, чтобы показать что $y \\in \\{-1, +1\\}.$\n",
    "\n",
    "$\\large L(y, b) = -[y_i = +1] \\log b - [y = -1] \\log (1-b)$\n",
    "\n",
    "### Задача 1\n",
    "\n",
    "Проверим следующую функцию потерь $L(y, b) = |[y = +1] - b|$, для этого необходимо посчитать математическое ожидание функции потерь по распределению $y$ в данной точке.\n",
    "\n",
    "$\\mathbb{E} \\biggl[|[y = +1] - b| \\biggr] = |1 - b| \\cdot p(y =+1 | x) + |0 - b| \\cdot (1 - p(y =+1 | x)) =$\n",
    "\n",
    "Обозначим как $p(y =+1 | x) = p$\n",
    "\n",
    "$= (1-b) \\cdot p + b \\cdot (1 - p)$\n",
    "\n",
    "$\\frac{\\partial}{\\partial b} \\mathbb{E} \\biggl[|[y = +1] - b| \\biggr] = -p + 1 - p = 1 -2p = 0$\n",
    "\n",
    "Получаем, что производная не зависит от $b$, значит функция константная либо функция линейная и минимум достигается на краях отрезка.\n",
    "\n",
    "Если $p(y =+1 | x) = \\frac{1}{2}$, тогда если в математическое ожидание функции потерь $\\mathbb{E} \\biggl[|[y = +1] - b| \\biggr]$ подставить $p = \\frac{1}{2}$, то и математическое ожидание будет равно $\\frac{1}{2}$. И поскольку, при подстановке любого $p$ математическое ожидание не изменяется, значит мы можем взять любое $b$ и оно будет оптимальным, значит функция потерь не гарантирует предсказание вероятностей.\n",
    "\n",
    "А если $p \\ne \\frac{1}{2}$, тогда минимум достигается на одном из краев отрезка (0 или 1), значит эта фукнция потерь не треуебт чтобы модель выдавала вероятность, она требует, чтобы модель выдавала, она требует чтобы модель выдавала 0 или 1.\n",
    "\n",
    "Поэтому данная функция не удовлетворяет требованию оценивания вероятностей."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8c615a",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#008B8B\">2. Калибровка вероятностей</h1>\n",
    "\n",
    "И возникает вопрос, как проверить, что модель оценивает вероятности?\n",
    "\n",
    "**Построение колибровочной кривой.** Строим график, где по оси $x$ расположены выходы модели $b(x)$ и отрезок разбит на бины, которых $m$ штук. А по оси $y$ откладывается доля действительно положительных объектов для этого бина.\n",
    "    \n",
    "Берем первый бин от $0$ до $0.1$,  выбираем все объекты от $0$ до $0.1$ и смотрим сколько среди этих объектов действительно положительных. И так делаем для каждого бина. Если классификатор идеально возвращает вероятноси, тогда кривая будет проходить по диагонали.\n",
    "    \n",
    "**ECE (expected calibration error)**\n",
    "\n",
    "$\\large \\text{ECE} = \\frac{1}{m}\\sum\\limits_{B \\in \\{B_1, \\ldots, B_m\\}}  \\biggl|\\frac{1}{|B|} \\sum\\limits_{(x, y) \\in B} b(x) - \\frac{1}{|B|} \\sum\\limits_{(x, y) \\in B} [y = + 1] \\biggl|$\n",
    "\n",
    "$\\frac{1}{m}\\sum\\limits_{B \\in B_1, \\ldots, B_m}$ - Сумма по всем бинам из множества бинов $B_1, \\ldots, B_m$ усредненное на $m$.\n",
    "\n",
    "$\\frac{1}{|B|} \\sum\\limits_{(x, y) \\in B} b(x)$ - в бине суммируем по всем парам $(x, y)$, которые находятся в бине. Так как необходимо посчитать долю положительных в бине и сравнить с выходами модели, тогда у нас получитя сумма по всем выходам модели на объектах в бине и деленное на размер бина и из этого мы вычитаем:\n",
    "\n",
    "$\\frac{1}{|B|} \\sum\\limits_{(x, y) \\in B} [y = + 1]$ - долю положительных объектов.\n",
    "\n",
    "Как бороться с зависимостью от разбиеня на бины?\n",
    "\n",
    "1) Разбиваем по количеству объектов, которые содержается в бине $|B_1| = |B_2| =, \\ldots, = |B_m|$.\n",
    "\n",
    "2) Использовать другие метрики:\n",
    "\n",
    "$\\large \\text{brierrose} = \\frac{1}{\\ell} \\sum\\limits_{i = 1}^{\\ell} (b(x_i) - [y_i = +1])^2$ - Минимизация данной функции приводит к коректному оцениванию вероятностей. Показывает насколько модель хорошо оценивает вероятности."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23740c3",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#008B8B\">2.1 Калибровка Платта (Параметрический)</h2>\n",
    "\n",
    "Имеется некоторая модель $C(x)$ и она выдает уверенности в прогнозе в любой шкале ($-\\infty$ до $+\\infty$). Зададим новую модель, которая устроена следующим образом:\n",
    "\n",
    "$\\large b(x) = \\frac{1}{1 + exp(\\alpha \\cdot C(x) + \\beta)}$ - функция, которая переводит выходы модели $C(x)$ на отрезок $[0, 1]$ и за счет $\\beta, \\alpha$ мы можем регулировать сдвиг и наклон кривой. Можно использовать не сигмойду, а любую другую праметрическую функцию, которая принимает выход некалиброванной модели $C(x)$.\n",
    "\n",
    "После того, как мы задали данную модель, мы можем обучить ее на $log-loss$.\n",
    "\n",
    "$\\large \\sum\\limits_{i = 1}^{\\ell} \\biggl(-[y_i = +1] \\log b(x_i) - [y_i = -1] \\log (1 - b(x_i)) \\biggl) \\to \\underset{\\alpha, \\beta}{\\text{min}}$\n",
    "\n",
    "**Почему обучать $b(x)$ и $\\alpha, \\beta$ на одной выборке плохо?**\n",
    "\n",
    "Мы обучаем модель на одном признаке $C(x)$, а он обучен уже под целевую переменную, значит содержит информацию \n",
    "\n",
    "Мы обучаем модель на одном признаке $C(x)$, но $C(x)$ при обучении использовала целевую переменную. Поэтому мы не можем обучать $b(x)$ по обучающей выборке. Так как модель $C(x)$ может хуже работать на тестовой выборке и распределение целевой переменной будет уже другим и новая модель $b(x)$ не сможет учесть этот факт. \n",
    "\n",
    "**Почему мы выполняем калибровку?**\n",
    "\n",
    "Модель с точки зрения классификации является хорошей, но она плохо оценивает вероятности, именно это мы клаибруем, добавляем ещё одно полезное свойство.\n",
    "\n",
    "**Почему логистическую регрессию надо калибровать?**\n",
    "\n",
    "Логистическая регрессия старается выдавать корректные оценки вероятности, но линейной модели может не хватать силы для этого. Если данные будут сложными и не линейными, тогда функция потерь будет требовать от модели корректной оценки вероятностей, но модель может оказаться слишком пройстой, чтобы выдавать корректные оценки веротяностей. Калибровка добавляет параметров в модель и поэтому после калибровки модель становится более сложной, параметризованной."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eef457",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#008B8B\">2.2 Изотоническая регрессия (Непараметрический)</h2>\n",
    "\n",
    "$\\large \\sum\\limits_{B_i \\in \\{B_1, \\ldots, B_m\\}} \\sum\\limits_{(x, y) \\in B} \\biggl(z_i - [y =+ 1] \\biggl)^2 \\to \\underset{z_i \\le z_2 \\le \\ldots \\le z_m}{\\text{min}}$ - Сумма по всем бинам, потом суммируем по всем объектам в бине и берем число $z_i$ и требуем чтобы оно было как можно ближе к правильному ответу на объекте. И требуем чтобы $z$ расположенные между бинами должны не убывать.\n",
    "\n",
    "Тем самым, для каждого бина мы находим оптимальное число $z_i$ и требуем, если бин идет раньше другого, то в нем $z_i$ должно быть меньше или равно чем в более позднем бине и эти $z_i$ должны как можно лучше приближали правильные ответы.\n",
    "\n",
    "Для обучения используется pool adjacent violators algorithm, который позволят подобрать оптимальные $z_i$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
