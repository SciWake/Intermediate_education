{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "236d837a",
   "metadata": {},
   "source": [
    "<h1 style=\"color:black\" align=\"center\">Семинар №9</h1>\n",
    "\n",
    "<h1 style=\"color:#008B8B\">1. Градиентный бустинг</h1>\n",
    "\n",
    "На лекциях обсуждался градиентный бустинг - один из самых мощных методов в машинном обучении. Он позволяет воспользоваться выразительной силой решающих деревьев и при этом контролировать их переобучение. Ниже мы пройдём по основным блокам градиентного бустинга и поймём, почему они устроены именно так.\n",
    "\n",
    "Для начала вспомним основные принципы градиентного бустинга. Будем искать алгоритм, оптимизирующий некоторую дифференцируемую функцию потерь $L(y, z)$, в виде взвешенной суммы базовых алгоритмов:\n",
    "\n",
    "$\\large a_N(x)\n",
    "=\n",
    "\\sum\\limits_{n = 0}^{N}\n",
    "    \\gamma_n b_n(x).$\n",
    "\n",
    "Как правило, веса $\\gamma_n$ не подбираются и полагаются равными единице (поскольку, как правило, в деревьях всё равно потом тщательно подбираются прогнозы в листьях), но пока будет рассматривать общий случай.\n",
    "\n",
    "Идея бустинга заключается в последовательном построении алгоритмов, каждый из которых учитывает ошибки построенной до сих пор композиции:\n",
    "\n",
    "$\\large \\sum\\limits_{i = 1}^{\\ell}\n",
    "    L(y_i, a_{N - 1}(x_i) + \\gamma_N b_N(x_i))\n",
    "\\to\n",
    "\\underset{b_N, \\gamma_N}{\\text{min}}$\n",
    "\n",
    "\n",
    "После выбора каких-нибудь простых $\\gamma_0$ и $b_0(x)$ (например, для задачи регрессии можно положить $\\gamma_0 = 1$ и $b_0(x) = \\frac 1\\ell \\sum\\limits_{i=1}^\\ell y_i$) все последующие базовые алгоритмы стараются приблизить антиградиент функционала ошибки, взятый в точках $z = a_{N - 1}(x_i)$:\n",
    "\n",
    "$\\large s_i\n",
    "=\n",
    "-\n",
    "\\left.\n",
    "\\frac{\\partial L(y_i, z)}{\\partial z}\n",
    "\\right|_{z = a_{N - 1}(x_i)}$\n",
    "\n",
    "\n",
    "При этом приближается антиградиент с точки зрения квадратичной функции потерь:\n",
    "\n",
    "$\\large b_N(x)\n",
    "=\n",
    "\\underset{b \\in \\mathbb{A}}{\\text{argmin}}\n",
    "    \\sum\\limits_{i = 1}^{\\ell}\n",
    "        \\left(\n",
    "            b(x_i) - s_i\n",
    "        \\right)^2$\n",
    "\n",
    "\n",
    "Подбор коэффициентов производится просто через задачу одномерной оптимизации:\n",
    "\n",
    "$\\large \\gamma_N\n",
    "=\n",
    "\\underset{\\gamma \\in \\mathbb{R}}{\\text{argmin}}\n",
    "\\sum\\limits_{i=1}^{\\ell}\n",
    "L(y_i, a_{N-1}(x_i) + \\gamma b_N (x_i))$\n",
    "\n",
    "\n",
    "Обсудим теперь подробнее некоторые моменты."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59b369f",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#008B8B\">1.1 Почему градиентный бустинг устроен именно так?</h2>\n",
    "\n",
    "### Зачем сдвиги в бустинге считаются через производные функции потерь?\n",
    "\n",
    "Почему нельзя использовать сдвиги вида $y_i - a_{N - 1}(x_i)$? Казалось бы, если удастся хорошо приблизить эти отклонения с помощью $b_N(x)$, то будет выполнено $a_{N-1}(x_i) + b_N(x_i) \\approx y_i$.\n",
    "\n",
    "Мы решаем сложные задачи, в которых, скорее всего, ещё и не идеальные данные, есть шумы, выбросы и так далее. Это означает, что не стоит рассчитывать на получение непереобученной модели с нулевой ошибкой на обучающей выборке. Мы это понимаем, и поэтому через функцию потерь определяем, какие ошибки более приемлемы, чем другие. Если использовать сдвиги, равные разности правильного ответа и прогноза текущей композиции, то мы полностью игнорируем эти требования. Разберём несколько примеров.\n",
    "\n",
    "**Пример 1.** Начнём с регрессии. Допустим, у нас несимметричная функция потерь, где мы сильнее штрафуем за завышение прогноза:\n",
    "\n",
    "$\\large L(y, z)\n",
    "=\n",
    "\\frac{1}{2}\n",
    "(10 [z \\geq y] + [z < y])\n",
    "(y - z)^2$\n",
    "\n",
    "Рассмотрим два объекта $x_1$ и $x_2$ с правильными ответами $y_1 = y_2 = 0$ и прогнозами текущей композиции $a_{N - 1}(x_1) = 5$ и $a_{N - 1}(x_2) = -5$. Если взять сдвиги, равные $y_i - a_{N - 1}(x_i)$, то базовая модель будет пытаться одинаково скорректировать прогнозы на обоих объектах. Но, конечно, надо сильнее сконцентрироваться на коррекции прогноза на первом объекте, поскольку на нём штраф больше: $L(0, 5) = 125 > 12.5 = L(0, -5)$. Сдвиги, посчитанные через частные производные, лучше отражают приоритеты: $s_1 = -50$, $s_2 = 5$. При этом не страшно, что на первом объекте мы сделаем слишком большую корректировку - это будет исправлено с помощью веса $\\gamma_N$ базовой модели или с помощью других техник."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67962ec",
   "metadata": {},
   "source": [
    "**Пример 2.** Обсудим теперь обучение на абсолютную ошибку $L(y, z) = |y - z|$.\n",
    "Вычислим для неё сдвиги:\n",
    "\n",
    "$\\large s_i = - \\left. \\frac{\\partial |y_i - z|}{\\partial \n",
    "    z} \\right|_{z=a_{N-1}(x_i)} = \n",
    "\\Bigl. \\text{sign}(y_i - z) \\Bigr|_{z=a_{N-1}(x_i)} =\n",
    "\\text{sign}(y_i - a_{N-1}(x_i))$\n",
    "\n",
    "И это снова более логично, чем обучение на $y_i - a_{N - 1}(x_i)$! Мы знаем, что с точки зрения квадратичной ошибки даже небольшое уменьшение серьёзной ошибки оказывается лучше, чем доведение небольшой ошибки до нуля:\n",
    "\n",
    "$\\large L(0, 1000) - L(0, 999) = 1999 > 1 = L(0, 1) - L(0, 0).$\n",
    "\n",
    "Как мы видим, при уменьшении прогноза тысячи на еденицу уменьшает ошибку на 1999, что оказывается лучше, чем доведение небольшой ошибки до нуля, так как ошибка изменится всего на еденицу. \n",
    "\n",
    "Поэтому для квадратичной ошибки разумно использовать сдвиги $y_i - a_{N - 1}(x_i)$, поскольку чем больше отклонение от правильного ответа, тем больше штраф и тем сильнее надо корректировать прогнозы. У абсолютной ошибки такого свойства нет, изменения штрафа не зависят от отклонения прогноза от факта. Поэтому независимо от того, на каком объекте мы приблизим прогноз к правильному ответу на единицу, средняя абсолютная ошибка улучшится на одну и ту же величину. У нас как раз получилось, что сдвиги на всех объектах одинаковы по модулю и отличаются лишь знаком - это соответствует описанному выше свойству абсолютной ошибки."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e1033d",
   "metadata": {},
   "source": [
    "**Пример 3.** Перейдём к классификации. Если у нас два класса ($\\mathbb{Y} = \\{-1, +1\\}$), и сама композиция также возвращает прогноз из $\\mathbb{Y}$, то отклонения всегда будут принимать значения из небольшого множества: $|y_i - a_{N - 1}(x_i)| \\in \\{0, +2\\}$. \n",
    "\n",
    "\n",
    "Получается, что если модель уже выдаёт правильный ответ, то сдвиги будут равню нулю, и никакие модификации для этого объекта вноситься не будут. Разумеется, это нас не устраивает - как правило, функции потерь в классификации стараются максимизировать отступ (или хотя бы довести его до определённого положительного значения), а полученные нами сдвиги никак не зависят от отступа.\n",
    "\n",
    "Чтобы это исправить, договоримся, что наша композиция $a_N(x)$ будет выдавать вещественные числа, и по смыслу они будут являться оценками *логита*, то есть логарифма отношения вероятности положительного класса к вероятности отрицательного класса:\n",
    "\n",
    "$\\large a_N(x)\n",
    "=\n",
    "\\log\n",
    "\\frac{\n",
    "    p(y = +1 | x)\n",
    "}{\n",
    "    1 - p(y = +1 | x)\n",
    "}$\n",
    "\n",
    "Если выразить отсюда вероятность, то получится\n",
    "\n",
    "$\\large p(y = +1 | x)\n",
    "=\n",
    "\\sigma\\left(\n",
    "    a_N(x)\n",
    "\\right)\n",
    "=\n",
    "\\frac{\n",
    "    1\n",
    "}{\n",
    "    1 + \\exp(-a_{N}(x))\n",
    "}$\n",
    "\n",
    "То есть сигмоида от модели будет оценивать вероятность положительного класса. Эту же идею мы использовали при обучении линейных классификаторов - там скалярное произведение как раз оценивало логиты. Получаем, что $a_N(x)$ - чем больше, тем выше уверенность в том, что объект положительный. С таким подходом обучение на $y_i - a_{N - 1}(x_i)$ выглядит ещё менее логичным, поскольку в этом случае мы будем пытаться уменьшить отступ там, где он большой положительный, то есть по сути будем запрещать модели быть уверенной в своём ответе. \n",
    "\n",
    "Например, имеется положительный равный $y_i = +1$ и построенна композиция выдает ответ на этом объекте $a_{N-1} (x_i) = 10000$, следовательно модель уверена в том, что этот объект положителен, у нее большой отступ на этом объекте. Отступ на этом объекте будет равен $y_i - a_{N-1} = -9999$, сдвиг говорит о том, что необходимо уменьшать прогноз модели, так как выход модели должен быть равен $+1$, а мы выдали $10000$. Следовательно, если использовать данные сдвиги, они будут запрещать строить уверенные модели, если модель уверена на некотором объекте, сдвиги будут уменьшать отступ объекта."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6e9be8",
   "metadata": {},
   "source": [
    "В завершение посчитаем сдвиги для логистической функции потерь:\n",
    "$L(y, z) = \\log (1 + \\exp(-yz))$. Для этого вспомним, что логистическая функция потерь выражается через сигмоиду $\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$ следующим образом:\n",
    "\n",
    "$\\large L(y, z) = \\log \\left( \\frac{1}{\\sigma(yz)} \\right) = -\\log \\sigma(yz)$\n",
    "\n",
    "Далее, пользуясь формулой для производной сигмоиды $\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$, получаем\n",
    "\n",
    "\\begin{align*}\n",
    "    s_i &= \\left. \\frac{\\partial \\log \\sigma(y_i z)}{\\partial z} \n",
    "        \\right|_{z=a_{N-1}(x_i)} =\n",
    "    \\left. \\frac{1}{\\sigma(y_i z)} \\sigma(y_i z) (1 - \\sigma(y_i z)) y_i\n",
    "        \\right|_{z=a_{N-1}(x_i)} =\\\\\n",
    "    &= (1-\\sigma(y_i a_{N-1}(x_i))) y_i =\n",
    "    \\frac{y_i}{1 + \\exp(y_i a_{N-1}(x_i))}.\n",
    "\\end{align*}\n",
    "\n",
    "Заметим, что чем больше отступ, тем меньше будут сдвиги по модулю. Здесь числитель говорит в какую сторону необходимо двигаться, для положитльных объектов числитель и знаменатель будет положителен, поэтому модель будет стремиться увеличивать прогноз композиции, для отрицательных объектов данная дробь будет отрицательной, поэтому прогноз композиции будет уменьшаться в сторону минус бесконечности. А знаметель говорит о том, чем больше отступ, тем меньше будет $|s_i|$. То есть, знаменатель говорит о том, что если на объекте уже хороший отступ, тогда сильно нет необходимости что-то изменять."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de6d124",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
