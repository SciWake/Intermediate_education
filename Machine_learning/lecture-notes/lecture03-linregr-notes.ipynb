{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecb02f1e",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#008B8B\">3. Обучение линейной регрессии</h1>\n",
    "\n",
    "Заметим, что здесь мы переписали выражение функционала ошибки, заменив $a(x)$ на скалярное призведение $\\left \\langle w,x \\right \\rangle$, после чего мы уже имеем функцию, а не функционал ошибки, так как $Q$ зависит не от некоторой функции $a(x)$, а от вектора весов $w$, и оптимизировать нужно именно по нему, что гораздо проще. Другими словами, в функционал ошибки необходимо подставить только веса, после чего произвести скалярное произведение весов на $x_i$ объект и получить предсказание модели.\n",
    "\n",
    "$\\large Q(w, X) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} (\\langle w, x_i \\rangle - y_i)^2 \\to \\underset{w}{\\text{min}}$\n",
    "\n",
    "Запишем эту задачу в матричном виде:\n",
    "\n",
    "$\\large X = \\begin{pmatrix}\n",
    "  x_{11} & x_{12} & \\ldots & x_{1d}\\\\\n",
    "  \\ldots & \\ldots & \\ldots & \\ldots\\\\\n",
    "  x_{\\ell 1} & x_{\\ell 2} & \\ldots & x_{\\ell d} \n",
    "\\end{pmatrix}$ - Матрица объекты-признаки\n",
    "\n",
    "$\\large y = \\begin{pmatrix}\n",
    "  y_{1}\\\\\n",
    "  \\ldots\\\\\n",
    "  y_{\\ell} \n",
    "\\end{pmatrix}$\n",
    "$\\large w = \\begin{pmatrix}\n",
    "  w_{1}\\\\\n",
    "  \\ldots\\\\\n",
    "  w_{d} \n",
    "\\end{pmatrix}$\n",
    "\n",
    "Теперь можно записать $\\text{MSE}$ в матричном виде:\n",
    "\n",
    "$\\large Q(w, X) = \\frac{1}{\\ell} \\|Xw - y\\|_2^2 \\to \\underset{w}{\\text{min}}$\n",
    "\n",
    "\n",
    "$\\large Xw = \\begin{pmatrix}\n",
    "  \\langle w, x_1 \\rangle\\\\\n",
    "  \\ldots\\\\\n",
    "  \\langle w, x_{\\ell} \\rangle \n",
    "\\end{pmatrix}$\n",
    "\n",
    "где используется евклидова ($L_{2}$) норма:\n",
    "\n",
    "$\\large ||x|| = \\sqrt{\\sum\\limits_{i=1}^{\\ell}{x_i^2}}$\n",
    "\n",
    "$\\large ||Xw-y|| = \\sqrt{\\sum\\limits_{i=1}^{\\ell}{(X_iw-y_i)^2}}$\n",
    "\n",
    "$\\large Q(w, X) = \\frac{1}{\\ell}\\sqrt{\\sum\\limits_{i=1}^{\\ell}{(X_iw-y_i)^2}} ^{2} = \\frac{1}{\\ell}\\sum\\limits_{i=1}^{\\ell}{(X_iw-y_i)^2}$\n",
    "\n",
    "\n",
    "**Найдём градиент:**\n",
    "\n",
    "$\\large \\nabla_{w}Q(w,X) = \\frac{2}{\\ell}X^{T}(Xw-y)$\n",
    "\n",
    "Приравняем выражение градиента к $0$ для получения аналитического решения:\n",
    "\n",
    "$\\large \\frac{2}{\\ell}X^{T}(Xw-y) = 0$\n",
    "\n",
    "$\\large X^{T}(Xw-y) = 0$\n",
    "\n",
    "$\\large X^{T}Xw-X^{T}y = 0$\n",
    "\n",
    "$\\large w = \\frac{X^{T}y}{X^{T}X}$\n",
    "\n",
    "$\\large w^* = (X^T X)^{-1} X^T y$ - (только если $X$ полного ранга)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5c3de0",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#008B8B\">4. Градиентный спуск и оценивание градиента</h1>\n",
    "\n",
    "<h2 style=\"color:#008B8B\">4.1 Градиентный спуск</h2>\n",
    "\n",
    "**1)** $\\large w^{(0)}$ - начальное приближение\n",
    "\n",
    "Дальше необходимо сместиться в сторону, куда функция убывает быстрее всего, для этого необходимо найти антиградиент функции в точке с начальным приближением и двигаться в сторону наискорейшего убывания функции, а после произвести замену весов.\n",
    "\n",
    "$\\large \\nabla_w Q(w^{(k-1)}, X)$ - значение градиента функции потерь в точке $w^{(k-1)}$\n",
    "\n",
    "**2)** $\\large w^{(k)} = w^{k-1} - \\eta_k \\nabla_w Q(w^{(k-1)}, X)$ - шаг градиентного спуска\n",
    "\n",
    "\n",
    "**3) Процесс остановки**\n",
    "\n",
    "1. $\\large \\|w^{(k)} - w^{(k-1)} \\| < \\epsilon$ - Если на $k$ шаге вектор весов мало отличается от вектора весов на $k-1$ шаге, тогда производится остановка.\n",
    "\n",
    "2. $\\large \\|Q(w^{(k)}, X) - Q(w^{(k-1)}, X)\\| < \\epsilon$ - Если ошибка на обучении не меняется с предыдущим шагом\n",
    "\n",
    "3. Когда ошибка на отложенной выборке перестает уменьшаться\n",
    "\n",
    "\n",
    "**4) Сходимость**\n",
    "\n",
    "$\\large \\nabla_w Q(w, X) \\approx 0$ - Гарантируется, что при некоторых условиях градиентый спуск сходится к точке где градиент равен нулю (Это может быть минимум, максимум или седловая точка). Если градиент обладает свойством липшицевости и длина шага подобрана не очень большой (длина шага зависит от липшицевой константы), то GD сойдется к локальному минимуму. \n",
    "\n",
    "Свойство липшицевости - $\\|\\nabla Q(x) - \\nabla Q(z)\\| \\le L \\cdot \\|x - z\\|$ -Разница градиента в точках $x, z$ меньше или равна константы $L$ умноженное на расстояние между $x, z$. Это свойство \"на пальцах\" означает, что если точки $x,z$ рядом, то и градиент в данных точка почти не отличается.\n",
    "\n",
    "Почти все функционалы ошибок для линейных моделей строго выпуклые, значит у данных функционалов одна точка и это точка глобального минимума. Для линейных моделей с матрицей полного ранга точка минимума будет одна и она будет являеться глобальной.\n",
    "\n",
    "Если решений несколько, тогда генерируем несколько начальных приближений и выбираем наименьшюу точку. Данный метод позволяет найти глобальный минимум, если имеются локальные минимумы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ef2097",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#008B8B\">4.2 Оценивание градиента</h2>\n",
    "\n",
    "## Полный градиент\n",
    "Функционал ошибки $\\text{MSE}$ для линейной модели принимает следующий вид:\n",
    "\n",
    "$\\large Q(w, X) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} (\\langle w, x_i \\rangle - y_i)^2$\n",
    "\n",
    "Как правило, в задачах машинного обучения функционал $Q(w, X)$ представим в\n",
    "виде суммы $\\ell$ функций:\n",
    "\n",
    "$\\large Q(w, X) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} q_i(w)$\n",
    "\n",
    "Пример того, чему равняется $q_i(w) = (\\langle w, x_i \\rangle - y_i)^2$.Проблема метода градиентного спуска (4.1) состоит в том, что на каждом шаге\n",
    "необходимо вычислять градиент всей суммы:\n",
    "\n",
    "$\\large \\nabla_w Q(w, X) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} \\nabla_w q_i(w)$ - Полный градиент"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2283bed1",
   "metadata": {},
   "source": [
    "## Стохастический градиентный спуск (SGD)\n",
    "\n",
    "Оценить градиент суммы функций можно градиентом одного случайно взятого\n",
    "слагаемого:\n",
    "\n",
    "$\\large \\nabla_w Q(w, X) \\approx \\nabla_w q_{i_k}(w)$, где $i_k$ — случайно выбранный номер слагаемого из функционала. В этом случае мы получим метод стохастического градиентного спуска:\n",
    "\n",
    "$\\large w^{(k)} = w^{(k - 1)} - \\eta_k \\nabla q_{i_k}(w^{(k - 1)})$\n",
    "\n",
    "\n",
    "У обычного градиентного спуска есть важная особенность: чем ближе текущая точка к минимуму, тем меньше в ней градиент, за счёт чего процесс замедляется и аккуратно попадает в окрестность минимума. В случае со стохастическим градиентным спуском это свойство теряется. \n",
    "\n",
    "\n",
    "На каждом шаге мы двигаемся в сторону, оптимальную с точки зрения уменьшения ошибки на одном объекте. Если полный градиент по всей обучающей выборке близок к нулю из этого не следует, что градиент для одного объекта тоже близок к нулю. Так как если некоторый набор весов оптимален для всей обучающей выборки, это не значит что этот же набор весов оптимален для оптимизации ошибки на одном объекте.\n",
    "\n",
    "Параметры, оптимальные для средней ошибки на всей выборке, не обязаны являться оптимальными для ошибки на одном из объектов. Поэтому SGD метод запросто может отдаляться от минимума, даже оказавшись рядом с ним. \n",
    "\n",
    "\n",
    "Чтобы исправить эту проблему, важно в SGD делать длину шага убывающей - тогда в окрестности оптимума мы уже не сможем делать длинные шаги и, как следствие, не сможем из этой окрестности выйти. Разумеется, потребуется выбирать формулу для длины шага аккуратно, чтобы не остановиться слишком рано и не уйти от минимума. В частности, сходимость для выпуклых дифференцируемых функций гарантируется (с вероятностью 1), если функционал удовлетворяет ряду условий (как правило, это выпуклость, дифференцируемость и липшицевость градиента) и длина шага удовлетворяет условиям Роббинса-Монро:\n",
    "\n",
    "Есть теорема, которая гласит, что если подобрать длину шага так, что ряд длин шагов расходился $\\sum\\limits_{k=1}^{\\infty} \\eta_k = \\infty$, а ряд квадратов длин шагов сходился $\\sum\\limits_{k=1}^{\\infty} \\eta_k^2 < \\infty$, тогда градиент обязательно сойдётся к минимуму (Условия Роббинса Монро). Тогда возникает вопрос, а какую длину шага использовать? (Методичка).\n",
    "\n",
    "Этим условиям, например, удовлетворяет шаг $\\eta_k = \\frac{1}{k}$.\n",
    "На практике сходимость с ним может оказаться слишком медленной,\n",
    "поэтому правильнее будет подбирать формулу для длины шага более аккуратно.\n",
    "\n",
    "**Заметка по скорости сходимости:**\n",
    "Для полного градиента расстояние между точками убывает как:\n",
    "\n",
    "$\\large Q(w^{(k)}) - Q(w^{*}) = O(\\frac{1}{k})$, где $Q(w^{*})$ -  точка оптимума.\n",
    "\n",
    "Для стохастического градиента:\n",
    "\n",
    "$\\large E[Q(w^{(k)}) - Q(w^{*})] = O(\\frac{1}{\\sqrt{k}})$\n",
    "\n",
    "В стохастическом градиенте скорость сходимости ниже, что требует больше шагов. Но один шаг в SGD выполняется быстрее, так как не надо каждый раз считать полный градиент. Поэтому стохастический градиент сходится быстрее полного градиента.\n",
    "\n",
    "**Критерии остановки:**\n",
    "\n",
    "1. Считаем функционал ошибки раз  в эпоху (полный проход по всей выборке). То есть, мы проходимся SGD по всей выборке, потомо проходимся по всей выборке ещё раз и сравниваем полный функционал ошибки.\n",
    "\n",
    "2. Оцениваем функционал ошибки $Q(w, X)$ или градиент $\\nabla Q(w, X)$ по k последним шагам. Например, усредняем ошибку на объектах по последним 100 шагам и смотрим как она изменяется.\n",
    "\n",
    "\n",
    "**Mini-batch GD**\n",
    "\n",
    "Можно повысить точность оценки градиента, используя несколько слагаемых\n",
    "вместо одного\n",
    "\n",
    "$\\large \\nabla_w Q(w, X) \\approx \\frac{1}{n} \\sum\\limits_{j = 1}^{n} \\nabla_w q_{i_{kj}}(w)$\n",
    "\n",
    "где $i_{kj}$- случайно выбранные номера слагаемых из функционала ($j$ пробегает значения от $1$ до $n$), а $n$ - параметр метода, размер пачки объектов для одного градиентного шага. С такой оценкой мы получим метод mini-batch gradient descent, который часто используется для обучения дифференцируемых моделей. \n",
    "\n",
    "С точки зрения теории, скорость сходимости Mini-batch такая же как у SGD, но на практике Mini-batch сходится быстрее SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703e9432",
   "metadata": {},
   "source": [
    "## Средний стохастический градиент (stochastic average gradient)\n",
    "\n",
    "В начале выбираем начальное приближение $w^{(0)}$, после этого необходимо посчитать градиент по все слогаемым из функционала, через $z_i^{(0)}$ обозначим градиенты отдельных слогаемых:\n",
    "\n",
    "$\\large z_i^{(0)} = \\nabla q_i(w^{(0)}), \\qquad i = 1, \\ldots, \\ell$\n",
    "\n",
    "На $k$-й итерации обновляем $z_i$ следующим образом: выбирается случайное слагаемое $i_k \\in \\{1, \\ldots, \\ell\\}$ и для него пересчитываем градиент. А для всех остальных объектов мы переносим оценку градиента с прошлого шага:\n",
    "\n",
    "$\\large z_i^{(k)} = \\begin{cases}\n",
    "    \\nabla q_i(w^{(k - 1)}),\n",
    "    \\quad\n",
    "    &\\text{если}\\ i = i_k;\\\\\n",
    "    z_i^{(k - 1)}\n",
    "    \\quad\n",
    "    & i \\ne i_k.\n",
    "\\end{cases}$\n",
    "\n",
    "Иными словами, пересчитывается один из градиентов слагаемых. Оценка градиента вычисляется как среднее вспомогательных переменных то есть мы используем все слагаемые, как в полном градиенте, но при этом почти все слагаемые берутся с предыдущих шагов, а не пересчитываются:\n",
    "\n",
    "$\\large \\nabla_w Q(w^{(k-1)}, X) \\approx \\frac{1}{\\ell} \\sum\\limits_{i = 1}^{\\ell} z_i^{(k)}$\n",
    "\n",
    "Наконец, делается градиентный шаг:\n",
    "\n",
    "$\\large w^{(k)} = w^{k-1} - \\eta_k \\nabla_w Q(w^{(k-1)}, X)$\n",
    "\n",
    "$\\large w^{(k)} = w^{(k - 1)} - \\eta_k \\frac{1}{\\ell} \\sum\\limits_{i = 1}^{\\ell} z_i^{(k)}$\n",
    "\n",
    "Данный метод имеет такой же порядок сходимости для выпуклых и гладких функционалов,\n",
    "как и обычный градиентный спуск:\n",
    "\n",
    "$\\large E[Q(w^{(k)}) - Q(w^*)] = O(\\frac{1}{k})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661b7097",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#008B8B\">5.3 Модификации градиентного спуска</h2>\n",
    "\n",
    "Вспомним как выглядит шаг \n",
    "\n",
    "$\\large w^{(k)} = w^{k-1} - \\eta_k \\nabla_w Q(w^{(k-1)}, X)$, а на месте $\\nabla_w Q(w^{(k-1)}, X)$ можно использовать оценку градиента заместо полного градиента.\n",
    "\n",
    "### Метод импульса (momentum)\n",
    "\n",
    "**Первая проблема градиентного спуска:**\n",
    "\n",
    "В случае, когда линии уровня вытянуты, тогда градиент будет ортоганален линиям уровня. Но проблема заключается в траектории оптимизации градиента, что тратит много времени.\n",
    "\n",
    "Если построить линии уровня, то мы увидим, что по оси $x$ мы движемся в одном направлении, а по оси $y$ происходят колебания вверх и вниз. Для устранения колебаний и сохранения поступательных движений вдоль оси $x$ необходимо просуммировать вектора и тогда осцилляции по оси $y$ сократят друг друга, а движение по оси $x$ останется.\n",
    "\n",
    "**Когда линии уровня могут быть вытянутыми?**\n",
    "\n",
    "Если один признак имеет большой масштаб, тогда небольшое изменение веса при признаки приводит к большому изменению модели. \n",
    "\n",
    "В нашем обсуждении линии уровня вытянуты вдоль охи $x$, следовательно признак, который расположен по осих $y$ имеет большой масштаб, так как по этому направлению линии уровня изменяются быстро. Если изменяется второй вес (по оси $y$), тогда ошибка изменяется сильно.\n",
    "\n",
    "**Реализация**\n",
    "\n",
    "В самом начале, необходимо инициализировать дополнительную переменную нулём:\n",
    "\n",
    "$\\large h_0 = 0$\n",
    "\n",
    "На следующем шаге пересчитаем переменную следующим образом: \n",
    "\n",
    "$\\large h_k = \\alpha h_{k - 1} + \\eta_k \\nabla_w Q(w^{(k-1)}, X)$\n",
    "\n",
    "Здесь $\\alpha \\in (0, 1)$ — параметр метода, определяющей скорость затухания градиентов с предыдущих шагов. $\\alpha h_{k - 1}$ - (экспоненциальное затухание), это значит, что значение прошлого шага вносит менший вклад, чем значение нового шага. Шаг будет выглядеть следующим образом:\n",
    "\n",
    "$\\large w^{(k)} = w^{(k-1)} - h_k$\n",
    "\n",
    "Получается, что в $h_k$ с некоторым затуханием $\\alpha$ мы накапливаем все пердыдущие градиенты, а после уже шагаем в сторону усреднённого градиента. Тем самым, мы шагаем не по градиенту на неком шаге, а шагаем по усредненному значению градиента предыдущих итераций."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e74367",
   "metadata": {},
   "source": [
    "## AdaGrad и RMSprop\n",
    "\n",
    "**Вторая проблема** (Для стохастического градиента)\n",
    "\n",
    "Иногда, бывает, что данные разреженные (некоторые признаки принимают нулевые значения). Например, когда мы используем \"One hot encoding\" происходит замена одного категориального признака на некоторое количество бинарных признаков и только один принимает значение равное 1. \n",
    "\n",
    "Стохастический градиентный спуск движется по одному обькту и если какой-то признак нулевой, тогда частная производная по его весу будет равна 0, значит вес при признаке не будет изменяться. \n",
    "\n",
    "Но, в стохастическом градиентном спуске длина шага убывает, значит, на тех признкаха где значение 0, вес не будет меняться, а на других шагах значение признака будут изменяться практически всегда. Проблема в том, что на каком-то признаке вес будет изменяться только в первый раз, а длина шага уже не максимальная.\n",
    "\n",
    "Пусть имеется разреженный признак $x_j$ - почти на всех объектах нулевой (One-hot encoding). В линейной модели данный признак будет иметь следующий вид:\n",
    "\n",
    "$\\large a(x) = \\ldots + w_j x_j + \\ldots$\n",
    "\n",
    "Если $x_j = 0$, тогда частная производная по $w_j$ тоже будет равна нулю:\n",
    "\n",
    "$\\large x_j = 0 \\Longrightarrow \\frac{\\partial Q}{\\partial w_j} = 0$ А это значит, что на большенстве шагов $w_j$ не будет изменяться. И проблема заключается в том, что $\\eta_k$ убывает по мере роста $k$. На начальных итерациях $\\eta_k$ имеет большое значение, так как мы можем $1000$ итераций не изменять вес $w_j$, а $\\eta_k$ будет убывать, то когда попадается не нулевой признак $w_j$, длина шаг может быть маленькой, так как мы прошли $1000$ итераций и не разу не изменили $w_j$.\n",
    "\n",
    "Если признаки редко принимают не нулевые значения, тогда необходимо для таких признаков изменять длину шага другим образом.\n",
    "\n",
    "### AdaGrad\n",
    "\n",
    "В методе AdaGrad предлагается сделать свою длину шага для каждой компоненты вектора параметров. При этом шаг будет тем меньше, чем более длинные шаги мы делали на предыдущих итерациях:\n",
    "\n",
    "$\\large G_{kj} = G_{k-1,j} + (\\nabla_w Q(w^{(k-1)}, X))_j^2$ Где $k$ - номер итерации. $j$ - номер признака.\n",
    "\n",
    "$\\large w_j^{(k)} = w_j^{(k-1)} - \\frac{\\eta_k}{\\sqrt{G_{kj} + \\epsilon}} (\\nabla_w Q(w^{(k-1)}, X))_j$\n",
    "\n",
    "$G_{kj}$ - Накапливается информация о том, насколько большой шаг по $w_j$ \n",
    "\n",
    "### RMSprop\n",
    "\n",
    "У метода AdaGrad есть большой недостаток: переменная $G_{kj}$ монотонно растёт,\n",
    "из-за чего шаги становятся всё медленнее и могут остановиться ещё до того,\n",
    "как достигнут минимум функционала.\n",
    "Проблема решается в методе RMSprop, где используется экспоненциальное затухание градиентов:\n",
    "\n",
    "$\\large G_{kj} = \\alpha G_{k-1,j} + (1 - \\alpha) (\\nabla_w Q(w^{(k-1)}, X))_j^2$\n",
    "\n",
    "В этом случае размер шага по координате зависит в основном от того, насколько\n",
    "быстро мы двигались по ней на последних итерациях.\n",
    "\n",
    "### Adam - совмешает momentum (инерцию) и AdaGrad (адаптивность)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
