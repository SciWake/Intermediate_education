{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb3226f4",
   "metadata": {},
   "source": [
    "<h1 style=\"color:black\" align=\"center\">Бэггинг, случайные леса и разложение ошибки на смещение и разброс</h1>\n",
    "\n",
    "<h1 style=\"color:#008B8B\">1. Бутстрап</h1>\n",
    "\n",
    "Рассмотрим простой пример построения композиции алгоритмов. Пусть дана конечная выборка $X = (x_i, y_i)_{i=1}^{\\ell}$ с вещественными ответами. Будем решать задачу линейной регрессии. Сгенерируем подвыборку с помощью *бутстрапа*. Равномерно возьмем из выборки $\\ell$ объектов с возвращением. Отметим, что из-за возвращения среди них окажутся повторы. Обозначим новую выборку через $X_1$. Повторив процедуру $N$ раз, сгенерируем $N$ подвыборок $X_1, \\dots, X_N$. Обучим по каждой из них линейную модель регрессии, получив *базовые алгоритмы* $b_1(x), \\dots, b_N(x)$.\n",
    "\n",
    "Предположим, что существует истинная функция ответа для всех объектов $y(x)$, а также задано распределение на объектах $p(x)$. В этом случае мы можем записать ошибку каждой функции регрессии\n",
    "\n",
    "$\\large \\varepsilon_j(x) = b_j(x) - y(x),\n",
    "\\qquad\n",
    "j = 1, \\dots, N,$\n",
    "\n",
    "и записать матожидание среднеквадратичной ошибки и это будет характеристикой качества $b_{j}(x)$ модели на всем пространстве объектов\n",
    "\n",
    "$\\large \\mathbb{E}_x (b_j(x) - y(x))^2\n",
    "=\n",
    "\\mathbb{E}_x \\varepsilon_j^2(x).$\n",
    "\n",
    "Средняя ошибка построенных функций регрессии имеет вид\n",
    "\n",
    "$\\large E_1\n",
    "=\n",
    "\\frac{1}{N}\n",
    "\\sum\\limits_{j = 1}^{N}\n",
    "\\mathbb{E}_x \\varepsilon_j^2(x).$\n",
    "\n",
    "Предположим, что ошибки несмещены (первое предположение) и некоррелированы (второе предположение), которое означет, что если $i$ модель ошиблась на некотором объекте, тогда из этого нельзя сделать предположения о $j$ модели на данном объекте, ошибается она или нет:\n",
    "\n",
    "$\\large \\begin{align*}\n",
    "    &\\mathbb{E}_x \\varepsilon_j(x) = 0;\\\\\n",
    "    &\\mathbb{E}_x \\varepsilon_i(x) \\varepsilon_j(x) = 0,\n",
    "    \\quad\n",
    "    i \\neq j.\n",
    "\\end{align*}$\n",
    "\n",
    "Построим теперь новую функцию регрессии,\n",
    "которая будет усреднять ответы построенных нами функций:\n",
    "\n",
    "$\\large a(x) = \\frac{1}{N} \\sum\\limits_{j = 1}^{N} b_j(x).$\n",
    "\n",
    "Найдем ее среднеквадратичную ошибку:\n",
    "\n",
    "$\\large \\begin{align*}\n",
    "    E_N\n",
    "    &=\n",
    "    \\mathbb{E}_x \\Biggl(\n",
    "        \\frac{1}{N} \\sum\\limits_{j = 1}^{n} b_j(x)\n",
    "        -\n",
    "        y(x)\n",
    "    \\Biggr)^2\n",
    "    =\\\\\n",
    "    &=\n",
    "    \\mathbb{E}_x \\Biggl(\n",
    "        \\frac{1}{N} \\sum\\limits_{j = 1}^{N} \\varepsilon_j(x)\n",
    "    \\Biggr)^2\n",
    "    =\\\\\n",
    "    &=\n",
    "    \\frac{1}{N^2}\n",
    "    \\mathbb{E}_x \\Biggl(\n",
    "        \\sum\\limits_{j = 1}^{N} \\varepsilon_j^2(x)\n",
    "        +\n",
    "        \\underbrace{\\sum\\limits_{i \\neq j} \\varepsilon_i(x) \\varepsilon_j(x)}_{=0}\n",
    "    \\Biggr)\n",
    "    =\\\\\n",
    "    &=\n",
    "    \\frac{1}{N} E_1.\n",
    "\\end{align*}$\n",
    "\n",
    "Мы получили, что математическое ожидание ошибки композииции в $N$ раз меньше чем математическое ожидание ошибки одной модели. Таким образом, усреднение ответов позволило уменьшить средний квадрат ошибки в $N$ раз!\n",
    "\n",
    "Следует отметить, что рассмотренный нами пример не очень применим на практике, поскольку мы сделали предположение о некоррелированности ошибок, что редко выполняется. Если это предположение неверно, то уменьшение ошибки оказывается не таким значительным. Позже мы рассмотрим более сложные методы объединения алгоритмов в композицию, которые позволяют добиться высокого качества в реальных задачах."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf51b634",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#008B8B\">2. Bias-Variance decomposition</h1>\n",
    "\n",
    "Этот инструмент позволяет понять из чего складывается ошибка моделей. Допустим, у нас есть некоторая выборка, на которой линейные методы работают лучше решающих деревьев с точки зрения ошибки на контроле. Почему это так? Чем можно объяснить превосходство определенного метода обучения? Оказывается, ошибка любой модели складывается из трех факторов: сложности самой выборки, сходства модели с истинной зависимостью ответов от объектов в выборке, и богатства семейства, из которого выбирается конкретная модель. Между этими факторами существует некоторый баланс, и уменьшение одного из них приводит к увеличению другого. Такое разложение ошибки носит название разложения на смещение и разброс, и его формальным выводом мы сейчас займемся.\n",
    "\n",
    "\n",
    "Пусть задана выборка $X = (x_i, y_i)_{i = 1}^{\\ell}$ с вещественными ответами $y_i \\in \\mathbb{R}$ (рассматриваем задачу регрессии). Будем считать, что на пространстве всех объектов и ответов $\\mathbb{X} \\times \\mathbb{Y}$ существует распределение $p(x, y)$, из которого сгенерирована выборка $X$ и ответы на ней. Другими словами, для любой пары (объекта, ответ) мы знаем вероятность того, что она попадется в выборке.\n",
    "\n",
    "Рассмотрим квадратичную функцию потерь\n",
    "\n",
    "$$\\large L(y, a)\n",
    "=\n",
    "\\bigl(\n",
    "    y - a(x)\n",
    "\\bigr)^2$$\n",
    "\n",
    "Поскольку мы знаем вероятность получить получения любой пары (объекта, ответ), тогда мы сможем посчитать для квадратичной функции потерь соответствующий ей *среднеквадратичный риск*\n",
    "\n",
    "$$\\large R(a)\n",
    "=\n",
    "\\mathbb{E}_{x, y}\\Bigl[\n",
    "    \\bigl(\n",
    "        y - a(x)\n",
    "    \\bigr)^2\n",
    "\\Bigr]\n",
    "=\n",
    "\\int_\\mathbb{X}\n",
    "\\int_\\mathbb{Y}\n",
    "    p(x, y)\n",
    "    \\bigl(\n",
    "        y - a(x)\n",
    "    \\bigr)^2\n",
    "dx\n",
    "dy.$$\n",
    "\n",
    "Простыми словами, мы берем все точки в пространстве и в конкретной точке считаем вероятность получить эту точку и умножаем на ошибку модели в этой точке. Тогда, если ошибка большая, но вероятность получить такую точку низкая, тогда это не внесет большого вклада.\n",
    "\n",
    "Данный функционал усредняет ошибку модели в каждой точке пространства $x$ и для каждого возможного ответа $y$, причём вклад пары $(x, y)$, по сути, пропорционален вероятности получить её в выборке $p(x, y)$. Другими словами, это способ посчитать ошибку модели если мы знаем распределение на всем пространстве объектов и ответов. Разумеется, на практике мы не можем вычислить данный функционал, поскольку распределение $p(x, y)$ неизвестно. Тем не менее, в теории он позволяет измерить качество модели на всех возможных объектах, а не только на обучающей выборке."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95939590",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76d217d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
