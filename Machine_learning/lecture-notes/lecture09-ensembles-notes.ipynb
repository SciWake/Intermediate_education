{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba6ea479",
   "metadata": {},
   "source": [
    "<h1 style=\"color:black\" align=\"center\">Бэггинг, случайные леса и разложение ошибки на смещение и разброс</h1>\n",
    "\n",
    "<h1 style=\"color:#008B8B\">1. Бутстрап</h1>\n",
    "\n",
    "Рассмотрим простой пример построения композиции алгоритмов. Пусть дана конечная выборка $X = (x_i, y_i)_{i=1}^{\\ell}$ с вещественными ответами. Будем решать задачу линейной регрессии. Сгенерируем подвыборку с помощью *бутстрапа*. Равномерно возьмем из выборки $\\ell$ объектов с возвращением. Отметим, что из-за возвращения среди них окажутся повторы. Обозначим новую выборку через $X_1$. Повторив процедуру $N$ раз, сгенерируем $N$ подвыборок $X_1, \\dots, X_N$. Обучим по каждой из них линейную модель регрессии, получив *базовые алгоритмы* $b_1(x), \\dots, b_N(x)$.\n",
    "\n",
    "Предположим, что существует истинная функция ответа для всех объектов $y(x)$, а также задано распределение на объектах $p(x)$. В этом случае мы можем записать ошибку каждой функции регрессии\n",
    "\n",
    "$\\large \\varepsilon_j(x) = b_j(x) - y(x),\n",
    "\\qquad\n",
    "j = 1, \\dots, N,$\n",
    "\n",
    "и записать матожидание среднеквадратичной ошибки и это будет характеристикой качества $b_{j}(x)$ модели на всем пространстве объектов\n",
    "\n",
    "$\\large \\mathbb{E}_x (b_j(x) - y(x))^2\n",
    "=\n",
    "\\mathbb{E}_x \\varepsilon_j^2(x).$\n",
    "\n",
    "Средняя ошибка построенных функций регрессии имеет вид\n",
    "\n",
    "$\\large E_1\n",
    "=\n",
    "\\frac{1}{N}\n",
    "\\sum\\limits_{j = 1}^{N}\n",
    "\\mathbb{E}_x \\varepsilon_j^2(x).$\n",
    "\n",
    "Предположим, что ошибки несмещены (первое предположение) и некоррелированы (второе предположение), которое означет, что если $i$ модель ошиблась на некотором объекте, тогда из этого нельзя сделать предположения о $j$ модели на данном объекте, ошибается она или нет:\n",
    "\n",
    "$\\large \\begin{align*}\n",
    "    &\\mathbb{E}_x \\varepsilon_j(x) = 0;\\\\\n",
    "    &\\mathbb{E}_x \\varepsilon_i(x) \\varepsilon_j(x) = 0,\n",
    "    \\quad\n",
    "    i \\neq j.\n",
    "\\end{align*}$\n",
    "\n",
    "Построим теперь новую функцию регрессии,\n",
    "которая будет усреднять ответы построенных нами функций:\n",
    "\n",
    "$\\large a(x) = \\frac{1}{N} \\sum\\limits_{j = 1}^{N} b_j(x).$\n",
    "\n",
    "Найдем ее среднеквадратичную ошибку:\n",
    "\n",
    "$\\large \\begin{align*}\n",
    "    E_N\n",
    "    &=\n",
    "    \\mathbb{E}_x \\Biggl(\n",
    "        \\frac{1}{N} \\sum\\limits_{j = 1}^{n} b_j(x)\n",
    "        -\n",
    "        y(x)\n",
    "    \\Biggr)^2\n",
    "    =\\\\\n",
    "    &=\n",
    "    \\mathbb{E}_x \\Biggl(\n",
    "        \\frac{1}{N} \\sum\\limits_{j = 1}^{N} \\varepsilon_j(x)\n",
    "    \\Biggr)^2\n",
    "    =\\\\\n",
    "    &=\n",
    "    \\frac{1}{N^2}\n",
    "    \\mathbb{E}_x \\Biggl(\n",
    "        \\sum\\limits_{j = 1}^{N} \\varepsilon_j^2(x)\n",
    "        +\n",
    "        \\underbrace{\\sum\\limits_{i \\neq j} \\varepsilon_i(x) \\varepsilon_j(x)}_{=0}\n",
    "    \\Biggr)\n",
    "    =\\\\\n",
    "    &=\n",
    "    \\frac{1}{N} E_1.\n",
    "\\end{align*}$\n",
    "\n",
    "Мы получили, что математическое ожидание ошибки композииции в $N$ раз меньше чем математическое ожидание ошибки одной модели. Таким образом, усреднение ответов позволило уменьшить средний квадрат ошибки в $N$ раз!\n",
    "\n",
    "Следует отметить, что рассмотренный нами пример не очень применим на практике, поскольку мы сделали предположение о некоррелированности ошибок, что редко выполняется. Если это предположение неверно, то уменьшение ошибки оказывается не таким значительным. Позже мы рассмотрим более сложные методы объединения алгоритмов в композицию, которые позволяют добиться высокого качества в реальных задачах."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0e8a01",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#008B8B\">2. Bias-Variance decomposition</h1>\n",
    "\n",
    "Этот инструмент позволяет понять из чего складывается ошибка моделей. Допустим, у нас есть некоторая выборка, на которой линейные методы работают лучше решающих деревьев с точки зрения ошибки на контроле. Почему это так? Чем можно объяснить превосходство определенного метода обучения? Оказывается, ошибка любой модели складывается из трех факторов: сложности самой выборки, сходства модели с истинной зависимостью ответов от объектов в выборке, и богатства семейства, из которого выбирается конкретная модель. Между этими факторами существует некоторый баланс, и уменьшение одного из них приводит к увеличению другого. Такое разложение ошибки носит название разложения на смещение и разброс, и его формальным выводом мы сейчас займемся.\n",
    "\n",
    "\n",
    "Пусть задана выборка $X = (x_i, y_i)_{i = 1}^{\\ell}$ с вещественными ответами $y_i \\in \\mathbb{R}$ (рассматриваем задачу регрессии). Будем считать, что на пространстве всех объектов и ответов $\\mathbb{X} \\times \\mathbb{Y}$ существует распределение $p(x, y)$, из которого сгенерирована выборка $X$ и ответы на ней. Другими словами, для любой пары (объекта, ответ) мы знаем вероятность того, что она попадется в выборке.\n",
    "\n",
    "Рассмотрим квадратичную функцию потерь\n",
    "\n",
    "$$\\large L(y, a)\n",
    "=\n",
    "\\bigl(\n",
    "    y - a(x)\n",
    "\\bigr)^2$$\n",
    "\n",
    "Поскольку мы знаем вероятность получить получения любой пары (объекта, ответ), тогда мы сможем посчитать для квадратичной функции потерь соответствующий ей *среднеквадратичный риск*\n",
    "\n",
    "$$\\large R(a)\n",
    "=\n",
    "\\mathbb{E}_{x, y}\\Bigl[\n",
    "    \\bigl(\n",
    "        y - a(x)\n",
    "    \\bigr)^2\n",
    "\\Bigr]\n",
    "=\n",
    "\\int_\\mathbb{X}\n",
    "\\int_\\mathbb{Y}\n",
    "    p(x, y)\n",
    "    \\bigl(\n",
    "        y - a(x)\n",
    "    \\bigr)^2\n",
    "dx\n",
    "dy.$$\n",
    "\n",
    "Простыми словами, мы берем все точки в пространстве и в конкретной точке считаем вероятность получить эту точку и умножаем на ошибку модели в этой точке. Тогда, если ошибка большая, но вероятность получить такую точку низкая, тогда это не внесет большого вклада.\n",
    "\n",
    "Данный функционал усредняет ошибку модели в каждой точке пространства $x$ и для каждого возможного ответа $y$, причём вклад пары $(x, y)$, по сути, пропорционален вероятности получить её в выборке $p(x, y)$. Другими словами, это способ посчитать ошибку модели если мы знаем распределение на всем пространстве объектов и ответов. Разумеется, на практике мы не можем вычислить данный функционал, поскольку распределение $p(x, y)$ неизвестно. Тем не менее, в теории он позволяет измерить качество модели на всех возможных объектах, а не только на обучающей выборке."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dc2cc5",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#008B8B\">2.1 Минимум среднеквадратичного риска</h2>\n",
    "\n",
    "У нас есть некоторая функция, которая показывает качество модели. Тогда какая модель будет оптимальной с точки зрения этой функции?\n",
    "\n",
    "$\\mathbb{E}_{x, y}\\Bigl[\n",
    "    \\bigl(\n",
    "        y - a(x)\n",
    "    \\bigr)^2\n",
    "\\Bigr]$\n",
    "\n",
    "У нас имеется математическое ожидание и мы хотим найти оптимальную функцию $a$, которая будет давать минимум для среднеквадратичного риска. Как это сделать?\n",
    "\n",
    "Можно переписать математическое ожидание в виде цепочки двух математических ожиданий - из условного математического ожидания и обычного математического ожидания:\n",
    "\n",
    "$\\mathbb{E}_{x, y}\\Bigl[\n",
    "    \\bigl(\n",
    "        y - a(x)\n",
    "    \\bigr)^2\n",
    "\\Bigr] \n",
    "=\n",
    "\\mathbb{E}_{x} \\mathbb{E}_{y}\\Bigl[\n",
    "    \\bigl(\n",
    "        y - a(x)\n",
    "    \\bigr)^2 | x\n",
    "\\Bigr]$\n",
    "\n",
    "Для упрощения выражения, можно добавить и вычесть условное математическое ожидание $\\mathbb{E}[y | x ]$. И если это подставить, раскрыть скобки... тогда эти математические ожидания переписывается в виде:\n",
    "\n",
    "Получаем, что функционал среднеквадратичного риска имеет вид\n",
    "\n",
    "$\\large R(a)\n",
    "=\n",
    "\\mathbb{E}_{x, y} (y - \\mathbb{E} (y | x))^2 +\n",
    "\\mathbb{E}_{x, y} (\\mathbb{E} (y | x) - a(x))^2.$\n",
    "\n",
    "У нас есть функционал $R(a)$, значит, мы можем найти, алгоритм на котором достигается минимум, с точки зрения функционала $R(a)$. От алгоритма $a(x)$ зависит только второе слагаемое, и оно достигает своего минимума, если $a(x) = \\mathbb{E} (y | x)$. Таким образом, оптимальная модель регрессии для квадратичной функции потерь имеет вид\n",
    "\n",
    "$\\large a_*(x) = \\mathbb{E} (y | x)\n",
    "=\n",
    "\\int_\\mathbb{Y} y p(y | x) dy.$\n",
    "\n",
    "Иными словами, мы должны провести \"взвешенное голосование\" по всем возможным ответам, причем вес ответа равен его апостериорной вероятности. Имеется точка $x$ и в этой точке имеется распределение ответов $p(y|x)$, тогда оптимальный ответ в этой точке математическое ожидание этого распределенния или средний ответ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c42173a",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#008B8B\">2.2 Ошибка метода обучения</h2>\n",
    "\n",
    "Для того, чтобы построить идеальную функцию регрессии, необходимо знать распределение на объектах и ответах $p(x, y)$, что, как правило, невозможно. Для обучения модели берется конечная выборка и по ней выбираем оптимальную модель. Другими словами, на практике вместо этого выбирается некоторый *метод обучения* $\\mu: (\\mathbb{X} \\times \\mathbb{Y})^\\ell \\to \\mathbb{A}$, который произвольной обучающей выборке ставит в соответствие некоторый алгоритм из семейства $\\mathbb{A}$. В качестве меры качества метода обучения можно взять усредненный по всем выборкам среднеквадратичный риск алгоритма, выбранного методом $\\mu$ по выборке:\n",
    "\n",
    "Для конкретной модели мы можем посчитать математическое ожидание как \n",
    "\n",
    "$\\mathbb{E}_{x, y} \\Bigl[\n",
    "    \\bigl(\n",
    "    y - a(x)\n",
    "    \\bigr)^2\n",
    "\\Bigr]$\n",
    "\n",
    "Но сейчас у нас нет конкретной модели, есть результат метода обучения на выборке и мы можем взять результат метода обучения применив его к обучающей выборке $X$, тогда мы получим модель, которая способна дать прогноз в любой точке:\n",
    "\n",
    "$\\mathbb{E}_{x, y} \\Bigl[\n",
    "    \\bigl(\n",
    "    y - \\mu(X)(x)\n",
    "    \\bigr)^2\n",
    "\\Bigr]$\n",
    "\n",
    "Тут берется математическое ожидание по всему кроме $x$, тогда усредним эти ошибки по всем возможным обучающим выборкам $X$ и это будет среднеквадратическим риском метода обучения:\n",
    "\n",
    "$L(\\mu)\n",
    "=\n",
    "\\mathbb{E}_{X} \\Bigl[\n",
    "    \\mathbb{E}_{x, y} \\Bigl[\n",
    "        \\bigl(\n",
    "        y - \\mu(X)(x)\n",
    "        \\bigr)^2\n",
    "    \\Bigr]\n",
    "\\Bigr]$ - Среднеквадратический риск метода обучения\n",
    "\n",
    "Другими словами, мы перебираем все возможные обучающие выборки, для каждой обучающей выборки строим соответствующую модель методом обучения и для этой модели считаем среднеквадратичный риск. \n",
    "\n",
    "Обучающая выборка случайная. Распределение на выборках $X$ имеет вид $p(X) = \\prod\\limits_{i = 1}^{\\ell} p(x_i, y_i)$, так как выборка независимо одинаково распределенная, поэтому мы перемножаем вероятности отдельных пар (объект, ответ), которые состоят в выборку, но приэтом размер выборки фиксирован $\\ell$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a335cc9c",
   "metadata": {},
   "source": [
    "**Подставим среднеквадратичный риск в формулу $L(\\mu)$**\n",
    "\n",
    "Обратим внимание, что результатом применения метода обучения $\\mu(X)$ к выборке $X$ является модель, поэтому правильно писать $\\mu(X)(x)$. Но это довольно громоздкая запись, поэтому будем везде дальше писать просто $\\mu(X)$, но не будем забывать, что это функция, зависящая от объекта $x$.\n",
    "\n",
    "$\\begin{align}\n",
    "    L(\\mu)\n",
    "    &=\n",
    "    \\mathbb{E}_{X} \\Bigl[\n",
    "        \\underbrace{\n",
    "            \\mathbb{E}_{x, y} \\Bigl[\n",
    "                \\bigl(\n",
    "                    y - \\mathbb{E}[y | x]\n",
    "                \\bigr)^2\n",
    "            \\Bigr]\n",
    "        }_{\\text{не зависит от $X$}}\n",
    "        +\n",
    "        \\mathbb{E}_{x, y} \\Bigl[\n",
    "            \\bigl(\n",
    "                \\mathbb{E}[y | x]\n",
    "                -\n",
    "                \\mu(X)\n",
    "            \\bigr)^2\n",
    "        \\Bigr]\n",
    "    \\Bigr]\n",
    "    =\\notag\\\\\n",
    "    &=\n",
    "    \\mathbb{E}_{x, y} \\Bigl[\n",
    "        \\bigl(\n",
    "            y - \\mathbb{E}[y | x]\n",
    "        \\bigr)^2\n",
    "    \\Bigr]\n",
    "    +\n",
    "    \\mathbb{E}_{x, y} \\Bigl[\n",
    "        \\mathbb{E}_{X} \\Bigl[\n",
    "            \\bigl(\n",
    "                \\mathbb{E}[y | x]\n",
    "                -\n",
    "                \\mu(X)\n",
    "            \\bigr)^2\n",
    "        \\Bigr]\n",
    "    \\Bigr]. \\label{eq:decomp1}\n",
    "\\end{align}$\n",
    "\n",
    "Так как первое слогаемое не зависит от $X$, перепишем его как есть. А второе слогаемо упростим и получим:\n",
    "\n",
    "$\\begin{align}\n",
    "    &L(\\mu)\n",
    "    =\n",
    "    \\underbrace{\n",
    "        \\mathbb{E}_{x, y} \\Bigl[\n",
    "            \\bigl(\n",
    "                y - \\mathbb{E}[y | x]\n",
    "            \\bigr)^2\n",
    "        \\Bigr]\n",
    "    }_{\\text{шум}}\n",
    "    +\\notag\\\\\n",
    "    &+\n",
    "    \\underbrace{\n",
    "        \\mathbb{E}_{x} \\Bigl[\n",
    "            \\bigl(\n",
    "                \\mathbb{E}_{X} \\bigl[ \\mu(X) \\bigr]\n",
    "                -\n",
    "                \\mathbb{E}[y | x]\n",
    "            \\bigr)^2\n",
    "        \\Bigr]\n",
    "    }_{\\text{смещение}}\n",
    "    +\n",
    "    \\underbrace{\n",
    "        \\mathbb{E}_{x} \\Bigl[\n",
    "            \\mathbb{E}_{X} \\Bigl[\n",
    "                \\bigl(\n",
    "                    \\mu(X)\n",
    "                    -\n",
    "                    \\mathbb{E}_{X} \\bigl[ \\mu(X) \\bigr]\n",
    "                \\bigr)^2\n",
    "            \\Bigr]\n",
    "        \\Bigr]\n",
    "    }_{\\text{разброс}}. \\label{eq:biasVarDecomp}\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e669a0",
   "metadata": {},
   "source": [
    "**Шум**\n",
    "\n",
    "Рассмотрим подробнее компоненты полученного разложения ошибки.\n",
    "Первая компонента характеризует *шум* в данных и равна ошибке идеального алгоритма. Невозможно построить алгоритм, имеющий меньшую среднеквадратичную ошибку. Вторая компонента характеризует *смещение (bias)* метода обучения, то есть отклонение среднего ответа обученного алгоритма от ответа идеального алгоритма. Третья компонента характеризует *дисперсию (variance)*, то есть разброс ответов обученных алгоритмов относительно среднего ответа. \n",
    "\n",
    "Берем все пары (объект, ответ) и сравниваем в каждой точке правильный ответ с прогнозом лучшей модели (мы выше выснили, что математическое ожидание $\\mathbb{E}[y | x]$ - это оптимальная модель в данной точке). Значи, первое слогаемо это ошибка на всех данных лучшей из возможных моделей. Это слогаемо измеряет насколько плоха лучшая из моделей и данное слогаемое не зависит от модели. Если в каждой точке $x$ распределение $y$ при условии $x$ выражденное (только один объект может быть в точке), тогда шум будет равен 0.\n",
    "\n",
    "**Смещение**\n",
    "\n",
    "Смещение показывает, насколько хорошо с помощью данных метода обучения и семейства алгоритмов можно приблизить оптимальный алгоритм. Как правило, смещение маленькое у сложных семейств (например, у деревьев) и большое у простых семейств (например, линейных классификаторов). Дисперсия показывает, насколько сильно может изменяться ответ обученного алгоритма в зависимости от выборки - иными словами, она характеризует чувствительность метода обучения к изменениям в выборке. Как правило, простые семейства имеют маленькую дисперсию, а сложные семейства - большую дисперсию.\n",
    "\n",
    "$\\mathbb{E}[y | x]$ - это оптимальная модель в данной точке.\n",
    "\n",
    "$\\mathbb{E}_{X} \\bigl[ \\mu(X) \\bigr]$ - это средний пргноз всех возможных моделей в этой точке. Мы взяли все возможные обучающие выборки $X_1, \\ldots$ размера $\\ell$, на каждой обучили модель $b_1(x)$ и усреднили их предсказание.\n",
    "\n",
    "Значит, это слогаемое показыват насколько всреднем модели откланяются от идеального прогноза. Тем самым, это некотарая характеристика того, насколько большое семейство моделей.\n",
    "\n",
    "**Разброс**\n",
    "\n",
    "В этом слогаемом мы берем конкретную выборку $X$ - $\\mathbb{E}_{X}$, берем на ней обученную модель конкретно на выборке $X$ - $\\mu(X)$ и смотрим на ее отклонение от средней модели $\\mathbb{E}_{X} \\bigl[ \\mu(X) \\bigr]$. Это слогаемое показыват, насколько взависимости от конкретной обучающей выборки может модель откланяться от средней. Чем больше значение этого слогаемого, тем сильнее выборка влияет на модель. Следовательно, слогаемое показывает чувствительность модели к изменениям в обучающей выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1793f279",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
