{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:black\" align=\"center\">Линейная регрессия</h1>\n",
    "\n",
    "<h2 style=\"color:#008B8B\">1. Регуляризация</h2>\n",
    "\n",
    "$\\large Q(w, X)$ - Функционал ошибки\n",
    "\n",
    "Заметим, что здесь мы переписали выражение функционала ошибки, заменив $a(x)$ на скалярное призведение $\\left \\langle w,x \\right \\rangle$, после чего мы уже имеем функцию, а не функционал ошибки, так как $Q$ зависит не от некоторой функции $a(x)$, а от вектора весов $w$, и оптимизировать нужно именно по нему, что гораздо проще.\n",
    "\n",
    "$\\large Q(w, X) + \\alpha \\|w\\|_2^2$ - регуляризованный функционал, где \n",
    "\n",
    "$\\left \\| w \\right \\|$ - регуляризатор, а $\\alpha$ - коэффициент регуляризации, который должен быть равен или больше нуля.\n",
    "\n",
    "Дальше все это минимизируем по весам:\n",
    "\n",
    "$\\large Q(w, X) + \\alpha \\|w\\|_2^2 \\to \\underset{w}{\\text{min}}$\n",
    "\n",
    "### Почему $w_0$ не входит в регуляризатор?\n",
    "\n",
    "При этом, в регуляризатор не должен входить свободный коэффициент $w_0$, так как целевая переменная может иметь большой масштаб и если регулиризовать $w_0$, тогда модель не сможет соответствовать этому масштабу (нельзя построить хорошую модель). \n",
    "\n",
    "### Зачем масштабировать признаки?\n",
    "\n",
    "Так же регуляризация может привести к странным результатам если признаки не масштабированы. Почему? Допустим, целевая переменная и признак $x_1$ имеет масштаб $10^6$, а второй признак имеет масштаб $1$:\n",
    "\n",
    "$\\large |y| \\approx 10^6 ~ |x_1| \\approx 10^6 ~ |x_2| \\approx 1$\n",
    "\n",
    "Дальше мы строим линейную модель:\n",
    "\n",
    "$\\large a(x) = w_0 + w_1 x_1 + w_2 x_2$\n",
    "\n",
    "Если второй признак является важным, он долже сильно влияет на значение целевой переменной. И чтобы признак оказывал большое влияние на целевую переменную, вес при признаке должен иметь масштаб $w_2 \\approx 10^6$. А вес при первом признаке не должен быть больше, так как сам признак имеет масштаб $10^6$ и вес при этом признаке будет порядка еденицы $w_1 \\approx 1$. Получается, что регуляризатор $\\|w\\|_2^2$ будет сильно штрафовать $w_2$ и не даст ему принять такой масштаб, но второй вес старается быть большим не из-за того, что он хочет переобучиться, ему необходимо иметь большой масштаб чтобы вносить большой вклад в целевую переменную. \n",
    "\n",
    "Из-за разницы масштабов, первому признаку будет легче иметь большой вклад в целевую переменную. Тем самым мы должны масштабировать признаки, иначе регуляризация будет штрафовать за большие веса там, где они должны быть большими."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Аналитическое решение для случая с регуляризацией:\n",
    "\n",
    "$\\large w = (X^T X + \\alpha I)^{-1} X^T y$ - (Для MSE)\n",
    "\n",
    "Благодаря добавлению диагональной матрицы к $X^T X$ данная матрица оказывается положительно определённой (все собественные значения $\\ge$ 0, к этим значениям добавляем еденичную матрицу и все собственные значения сдвигаются на $\\alpha$, так как собственные значения изначально были не отрицательными, мы их увеличили на $\\alpha$ и собственные значения стали положительные, а это значит, что матрица точно не вырожденная), и поэтому её можно обратить. Таким образом, при\n",
    "использовании $L_2$ регуляризации решение всегда будет единственным.\n",
    "\n",
    "### Почему большие веса это плохо?\n",
    "\n",
    "Регуляризация позволяет ограничить большие веса модели, поэтому её стоит использовать. Главная проблема больших весов заключается в том, что при небольшом изменении, например, площади квартиры прогноз изменится на огромную сумму. Тем самым, модель с регуляризацией становится менее чувствительной к небольшим изменениям значения признаков.\n",
    "\n",
    "**Пример:**\n",
    "\n",
    "$\\large a(x) = 10^6 + 3 \\cdot 10^7 \\cdot \\text{(Площадь)} - 5 \\cdot 10^6 \\cdot \\text{Расстояние до метро}$\n",
    "\n",
    "Так как модель переобученная у нее большие веса. Если увеличить площадь на $\\text{(Площадь)} + 0.001$ квадраных метров, тогда прогноз модели изменится $a(x) + 3\\cdot 10^4$ или же на $30000$, что является странным поведением модели, так как это не должно было изменить прогноз модели.\n",
    "\n",
    "### Как выбрать $\\alpha$?\n",
    "\n",
    "По обучающей выборке, выбираем $\\alpha$ при которой ошибка модели $Q(w, X)$ на обучающей выборке минимальна. Данный подход является плохим, так как если $\\alpha$ не нулева, тогда добавляется требование, чтобы веса были меньше. За счет чего веса будут меньше? За счет увеличения ошибки на обучающей выборке. \n",
    "\n",
    "Оптимальная модель на обучающей выборке достигается при решении следующей задачи:\n",
    "\n",
    "$\\large Q(w, X) \\to \\underset{w}{\\text{min}}$\n",
    "\n",
    "А когда мы решаем задачу с положительным регуляризато - мы получим веса при которых ошибка на обучающей выборке будет больше, так как минимум достигается без регуляризатора, с регуляризатором решение будет другим и ошибка не может быть меньше.\n",
    "\n",
    "$\\large Q(w, X) + \\alpha \\|w\\|_2^2 \\to \\underset{w}{\\text{min}}$\n",
    "\n",
    "Поэтому с точки зрения ошибки на обучающей выборке - оптимально брать $\\alpha = 0$. Такие параметры, которые нельзя подбирать по обучающей выборке, так как они вводятся в модель для оптимизации качества на новых данных и называюстя гиперпараметрами. Делаем вывод, что гиперпараметры необходимо подбирать по отложенной выборке или по кросс-валидации. \n",
    "\n",
    "Ещё одинм примером гиперпараметров является количество степеней признаков в полиномиальной регрессии, так как с точки зрения обучающей выборки - чем больше степеней признаков мы добавим, тем лучше модель сможет подогнаться под обучающую выборку. Поэтому подбирать данный параметр на обучающей выборке будет не разумным, оптимальное количество степеней будет стремиться к бесконечности, разумнее подобрать этот гиперпараметр по отложенной выборке.\n",
    "\n",
    "В этой задаче мы хотим чтобы веса были небольшими и ошибка была минимальной. $\\alpha$ определяет что важнее, если $\\alpha$ имеет маленькое значение, тогда нам важнее подгнаться под обучающую выборку, так как с уменьшением $\\alpha$ мы приближаемся к задаче минимизации фукционала ошибки на обучающей выборке без регуляризатора."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Линейная зависимость признаков\n",
    "\n",
    "Пусть в выборке есть линейно зависимые признаки. Это по определению означает, что существует такой вектор $v$, что для любого объекта $x$ выполнено $\\langle v, x \\rangle = 0$.\n",
    "\n",
    "Допустим, мы нашли оптимальный вектор весов $w^*$ для линейного классификатора. Но тогда классификаторы с векторами $w^* + \\alpha v$ будут давать точно такие же ответы на всех объектах, поскольку:\n",
    "\n",
    "$\\large \\langle w^* + \\alpha v, x \\rangle = \\langle w^*, x \\rangle + \\alpha \\underbrace{\\langle v, x \\rangle}_{=0} = \\langle w^*, x \\rangle$\n",
    "\n",
    "Модель с данным набором весов $w^* + \\alpha v$ даёт такие же прогнозы как у линейной модели с набором весов $w^*$. Так как две модели дают одинаковые прогнозы, значит $MSE$ у моделей равны, тогда у знадачи оптимизации решений много. \n",
    "\n",
    "Основная проблема заключается в том, что если взять веса $w^* + \\alpha v$ и $\\alpha$ устремить в бесконечность, то мы будем с ростом $\\alpha$ получать решения, где веса будут всё больше и больше. Данные решения дадут одинаковый результат предсказания на обучающей выборке, но на отложенной выборке мы получим переобучение, так как веса имеют огромные значения.\n",
    "\n",
    "Решением этой проблемы становится регуляризация. С точки зрения $MSE$ все эти наборы весов одинаковы, но с точки зрения $L_2$ нормы есть решения с наименьшими весами. Значит, регуляризованный $MSE$ после минимизации способен подобрать веса с наилучшей ошибкой и при этом, веса будут иметь небольшие значения по модулю."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#008B8B\">2. Разреженные модели</h2>\n",
    "\n",
    "### $L_1$ регуляризация:\n",
    "\n",
    "$\\large Q(w, X) + \\alpha \\|w\\|_1 \\to \\underset{w}{\\text{min}}$\n",
    "\n",
    "У $L_1$ регуляризатора есть свойсво, что чем больше $\\alpha$, тем больше будет нулевых весов в итоговм векторе. Такое не происходит с $L_2$ регуляризатором. Когда производится зануление части весов - это значит, что мы пытаемся сделать разреженную моделью. Зачем это делают?\n",
    "\n",
    "1. Увеличение скорости - (выбрасываем веса, которые зануляются);\n",
    "\n",
    "2. Убираем признаки, которые не имеют пользы;\n",
    "\n",
    "3. Решение проблемы, когда данных меньше признаков.\n",
    "\n",
    "### Почему $L_1$ регуляризатор отбирает признаки?\n",
    "\n",
    "**Объяснение 1. Угловые точки (Теорема Куна — Таккера)**\n",
    "\n",
    "Решение данной задачи $\\large Q(w, X) + \\alpha \\|w\\|_1 \\to \\underset{w}{\\text{min}}$ эквивалетно решению условной задачи оптимизации:\n",
    "\n",
    "$\\left\\{\n",
    "    \\begin{aligned}\n",
    "        & Q(w, X) \\to \\min_{w} \\\\\n",
    "        & \\|w\\|_1 \\leq C\n",
    "    \\end{aligned}\n",
    "\\right.$\n",
    "\n",
    "Данные задачи эквиваленты, если правильно подобрать значение $C$ под $\\alpha$. Получается, что регуляризация - это просто ограничение множества возможных решений, другими словами, мы минимизируем тот же функционал, только веса должны быть из некоторой окресности нуля (второе ограничение). Если построить линии уровня функционала качества, а так же ограничения, задаваемы $L_2$ или $L_1$ регуляризаций, тогда при условии выше, с геометрической точки зрения ограничения буду иметь вид ромба (мы ищем решение внутри ромба) и линия уровня будет пересекаться с большей вероятностью на верхушке ромаб, тогда один признак на графике принимает значение равное 0. А если в задаче выше подставим $L_2$ регуляризацию:\n",
    "\n",
    "$\\left\\{\n",
    "    \\begin{aligned}\n",
    "        & Q(w, X) \\to \\min_{w} \\\\\n",
    "        & \\|w\\|_2^2 \\leq C\n",
    "    \\end{aligned}\n",
    "\\right.$\n",
    "\n",
    "Тогда ограничения будут иметь форму круга и пересечение произойдет скорее всего сбоку."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Объяснение 2. Штрафы при малых весах**\n",
    "\n",
    "Имеется вектор весов $w = (1, \\epsilon)$. \n",
    "\n",
    "Уменьшим первую координату на $\\delta$, где $0 < \\delta < \\epsilon < 1$ и посмотрим как изенится норма:\n",
    "\n",
    "$\\large \\|w - (\\delta, 0)\\|_2^2 = (1 - \\delta)^2 + \\epsilon^2 = 1 - 2\\delta + \\delta^2 + \\epsilon^2$\n",
    "\n",
    "$\\large \\|w - (\\delta, 0)\\|_1  = 1 - \\delta + \\epsilon$\n",
    "\n",
    "Уменьшим вторую координату на $\\delta$, где $\\delta < \\epsilon$ и посмотрим как изенится норма:\n",
    "\n",
    "$\\large \\|w - (0, \\delta)\\|_2^2 = 1^2 + (\\epsilon - \\delta)^2 = 1 - 2\\epsilon \\delta + \\delta^2 + \\epsilon^2$\n",
    "\n",
    "$\\large \\|w - (0, \\delta)\\|_1  = 1 - \\delta + \\epsilon$\n",
    "\n",
    "С точки зрения $L_1$ нормы нет разницы какую координату уменьшаем. Значит и нет разницы какие по размеру веса мы изменяем, большие или маленькие. \n",
    "\n",
    "$L_2$ норма меньше в первом случае, чем во втором. Получается, что с точки зрения $L_2$ нормы выгоднее изменять большие веса, чем мальнькие, так в этом случае ошибка будет уменьшаться быстрее всего. Поэтому, если имеется маленький вес, то $L_2$ норма его не будет уменьшать, ей выгоднее уменьшить большие веса, нежели маленькие.\n",
    "\n",
    "А для $L_1$ нормы нет разницы что уменьшать, так как ошибка изменяется равнозначно как у больших так и у маленьких весов."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
