{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:black\" align=\"center\">Линейная классификация</h1>\n",
    "\n",
    "<h1 style=\"color:#008B8B\">1. Линейные модели классификации</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть $\\mathbb{X} = \\mathbb{R}^n$ пространство объектов.\n",
    "\n",
    "$\\mathbb{Y} = \\{-1, +1\\}$ - множество допустимых ответов. \n",
    " * Если $y = 1$ - положительный объект\n",
    " * $y = -1$ - отрицательный объект.\n",
    "\n",
    "$X = \\{(x_i, y_i)\\}_{i = 1}^{\\ell}$ - обучающая выборка.\n",
    "\n",
    "Линейная модель имеет следующий вид: \n",
    "\n",
    "$\\large a(x) = sign (\\langle w, x \\rangle + w_0)$\n",
    "\n",
    "Уравнение $\\langle w, x \\rangle + w_0 = 0$ определяет гиперплоскость у которой $w$ - это вектор нормали.\n",
    "\n",
    " * Если $\\langle w, x \\rangle + w_0 = 0$ тогда точка лежит на гиперплоскости;\n",
    " * Если $\\langle w, x \\rangle + w_0 > 0$ тогда точка лежит на одной из полуплоскостей;\n",
    " * Если $\\langle w, x \\rangle + w_0 > 0$ тогда точка лежит на второй полуплоскости.\n",
    "\n",
    "<img src=\"img/4_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#008B8B\">1.1 Обучение линейных классификаторов</h2>\n",
    "\n",
    "Функционал ошибки будет выглядеть следующим образом:\n",
    "\n",
    "$\\large Q(a, X) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} [a(x_i) = y_i]$ - долей правильных ответов (accuracy)\n",
    "\n",
    "Нам будет удобнее решать задачу минимизации, поэтому будем вместо этого использовать долю неправильных ответов:\n",
    "\n",
    "$\\large Q(a, X) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} [a(x_i) \\ne y_i]$ - доля ошибок.\n",
    "\n",
    "Если не сказано иначе, мы будем считать, что среди признаков есть константа, $x_{d + 1} = 1$. В этом случае нет необходимости вводить сдвиг $w_0$, и линейный классификатор можно задавать как\n",
    "\n",
    "$\\large a(x) = sign (\\langle w, x \\rangle)$\n",
    "\n",
    "Подставим в функцию потерь модель:\n",
    "\n",
    "$\\large \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} [sign (\\langle w, x \\rangle) \\ne y_i] \\to \\underset{w}{\\text{min}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Как оптимизировать функционал?**\n",
    "\n",
    "Как мы решаем задачи оптимизации? Можем найти точное решение посчитав градиент или использовать градиентный спуск, где в двух случаях необходимо посчитать частные производные функционала по вектору весов. Но, веса расположены под знаком $sign$, который не является дифференцируемой функцией и всё это расположено внутри идникатора, который тоже не является дифференцируемой функцией.\n",
    "\n",
    "**Выполним преобразование функционала ошибки**\n",
    "\n",
    "$\\large \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} [y_i \\langle w, x \\rangle < 0] \\to \\underset{w}{\\text{min}}$\n",
    "\n",
    "Утвержается, что стоит под индекатором $sign (\\langle w, x \\rangle) \\ne y_i$ эквивалентно $y_i \\langle w, x \\rangle < 0$.\n",
    "\n",
    "$\\large y_i \\langle w, x \\rangle > 0$ означает, что $y_i$ и $\\langle w, x \\rangle$ одного знака, а если они одного знака, тогда мы правильно угадали класс. Значит, ответ верный.\n",
    "\n",
    "$\\large y_i \\langle w, x \\rangle < 0$ в данном случае, значение слева меньше нуля, значит $y_i$ и $\\langle w, x \\rangle$ разного знака и ответ неверный."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Отступы\n",
    "\n",
    "Заметим, что функционал (1.1) можно несколько видоизменить:\n",
    "\n",
    "$\\large Q(a, X) = \\frac{1}{\\ell} \\sum_{i = 1}^{\\ell} [\\underbrace{y_i \\langle w, x_i \\rangle}_{M_i} < 0] \\to \\underset{w}{\\text{min}}$\n",
    "\n",
    "$\\large M_i = y_i \\langle w, x \\rangle$ - отступ (margin)\n",
    "\n",
    "Знак отступа говорит о корректности ответа классификатора (положительный отступ соответствует правильному ответу, отрицательный — неправильному), а абсолютная величина отступа (напримр, большой отступ или маленький по модулю) характеризует степень уверенности классификатора в своём ответе.\n",
    "\n",
    "Абсолютное значение отступа $\\large |M_i|$ это расстояние от $x_i$ до разделяющей гиперплоскости. Если $L_2$ норма вектора весов равна $1$, тогда отступ является расстоянием до разделяющей гиперплоскости. Если $L_2$ норма вектора весов не равна $1$, тогда это отмасштабированное расстояние. Но по идеии, чем больше значение отступа, тем больше расстояние до разделяющей гиперплоскости, значит и уверенность классификатора в своём ответе будет выше.\n",
    "\n",
    "Если модель уверенна в своём ответе на некотором объекте и она ошиблась, тогда объект скорее всего является выбросом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
