{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:black\" align=\"center\">Линейная регрессия</h1>\n",
    "\n",
    "<h2 style=\"color:#008B8B\">1. Регуляризация</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large Q(w, X)$ - Функционал ошибки\n",
    "\n",
    "Заметим, что здесь мы переписали выражение функционала ошибки, заменив $a(x)$ на скалярное призведение $\\left \\langle w,x \\right \\rangle$, после чего мы уже имеем функцию, а не функционал ошибки, так как $Q$ зависит не от некоторой функции $a(x)$, а от вектора весов $w$, и оптимизировать нужно именно по нему, что гораздо проще.\n",
    "\n",
    "$\\large Q(w, X) + \\alpha \\|w\\|_2$ - регуляризованный функционал, где \n",
    "\n",
    "$\\left \\| w \\right \\|$ - регуляризатор, а $\\alpha$ - коэффициент регуляризации, который должен быть равен или больше нуля. При этом, в регуляризатор не должен входить свободный коэффициент $w_0$. Дальше все это минимизируем по весам:\n",
    "\n",
    "$\\large Q(w, X) + \\alpha \\|w\\|_2 \\to \\min_{w}$\n",
    "\n",
    "**Аналитическое решение для случая с регуляризацией:**\n",
    "\n",
    "$\\large w = (X^T X + \\alpha I)^{-1} X^T y$ - (Для MSE)\n",
    "\n",
    "Благодаря добавлению диагональной матрицы к $X^T X$ данная матрица оказывается положительно определённой, и поэтому её можно обратить. Таким образом, при\n",
    "использовании $L_2$ регуляризации решение всегда будет единственным.\n",
    "\n",
    "Регуляризация позволяет ограничить большие веса модели, поэтому её стоит использовать. Главная проблема больших весов заключается в том, что при небольшом изменении, например, площади квартиры прогноз изменится на огромную сумму. Тем самым, модель с регуляризацией становится менее чувствительной к небольшим изменениям значения признаков.\n",
    "\n",
    "**$L_1$ регуляризация:**\n",
    "\n",
    "$\\large Q(w, X) + \\alpha \\|w\\|_1 \\to \\min_{w}$\n",
    "\n",
    "$L_1$ регуляризатор способен занулять некоторые веса, чем больше $\\alpha$, тем больше весов будут зануляться. Такое не происходит с $L_2$ регуляризатором.\n",
    "\n",
    "**Линейная зависимость признаков**\n",
    "\n",
    "Пусть в выборке есть линейно зависимые признаки. Это по определению означает, что существует такой вектор $v$, что для любого объекта $x$ выполнено $\\langle v, x \\rangle = 0$.\n",
    "\n",
    "Допустим, мы нашли оптимальный вектор весов $w$ для линейного классификатора.\n",
    "Но тогда классификаторы с векторами $w + \\alpha v$ будут давать точно такие же\n",
    "ответы на всех объектах, поскольку:\n",
    "\n",
    "$\\large \\langle w + \\alpha v, x \\rangle = \\langle w, x \\rangle + \\alpha \\underbrace{\\langle v, x \\rangle}_{=0} = \\langle w, x \\rangle$\n",
    "\n",
    "Модель с данным набором весов $w + \\alpha v$ даёт такие же прогнозы как у линейной модели с набором весов $w$. Так как две модели дают одинаковые прогнозы, значит $MSE$ у моделей равны, тогда у знадачи оптимизации решений много. \n",
    "\n",
    "Основная проблема заключается в том, что если взять веса $w + \\alpha v$ и $\\alpha$ устремить в бесконечность, то мы будем с ростом $\\alpha$ получать решения, где веса будут всё больше и больше. Данные решения дадут одинаковый результат предсказания на обучающей выборке, но на отложенной выборке мы получим переобучение, так как веса имеют огромные значения.\n",
    "\n",
    "Решением этой проблемы становится регуляризация. С точки зрения $MSE$ все эти наборы весов одинаковы, но с точки зрения $L_2$ нормы есть решения с наименьшими весами. Значит, регуляризованный $MSE$ после минимизации способен подобрать веса с наилучшей ошибкой и при этом, веса будут иметь небольшие значения по модулю."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
