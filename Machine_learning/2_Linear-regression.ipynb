{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:black\" align=\"center\">Линейная регрессия</h1>\n",
    "\n",
    "<h1 style=\"color:#008B8B\">1. Линейные модели</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такие модели сводятся к суммированию значений признаков с некоторыми весами:\n",
    "\n",
    "$\\large a(x) = w_0 + \\sum\\limits_{j=1}^d w_j x_j$\n",
    "\n",
    "Параметрами модели являются веса или коэффициенты $w_j$. Вес $w_0$ также называется\n",
    "свободным коэффициентом или сдвигом (bias).\n",
    "\n",
    "Добавим фиктивный признак равный $1$, тогда запись можно упросить до следующего вида:\n",
    "\n",
    "$\\large a(x) = \\sum\\limits_{j=0}^d w_j x_j$\n",
    "\n",
    "$\\large a(x) = \\langle w, x \\rangle$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#008B8B\">2. Область применимости линейных моедлей</h1>\n",
    "\n",
    "Сформулируем задачу: \n",
    "\n",
    "$x$ - кваритра в Москве;\n",
    "\n",
    "$y$ - рычночная стоимость.\n",
    "\n",
    "$\\large a(x) = w_0 + w_1 \\text{(площадь)} + w_2\\text{(количество комнат)} + w_2 \\text{(расстояние до метро)}$\n",
    "\n",
    "Проблема модели заключается в том, что признаки независимо влияют на стоимость квартиры. Если увеличиваетя площадь квартиры, то цена увеличивается только засчёт площади квартиры в линейном виде, при этом растрояние до метро и все остальные признаки не оказывают влияния.\n",
    "Например, если квартира расположена близко от метро, то цена должна увеличиваться быстрее при росте площади."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Категориальные признаки\n",
    "\n",
    "$x_j$ - категориальные признаки\n",
    "\n",
    "$C = \\{c_1, \\ldots, c_m\\}$ - множество значений признака\n",
    "\n",
    "Заменим на $m$ бинарных признаков $b_1(x), \\ldots, b_m(x)$, где $b_i(x) = [x_j = c_i]$.\n",
    "\n",
    "При этом, признаки $b_1(x), \\ldots, b_m(x)$ являются линейно зависимыми: \n",
    "\n",
    "$b_1(x), \\ldots, b_m(x) = 1$\n",
    "\n",
    "И модель принимает следующий вид:\n",
    "\n",
    "$\\large a(x) = w_0 + w_1 [x_j = c_1] + \\ldots + w_m [x_j = c_m] + \\text{\\{Взаимодействие других признаков\\}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Бинаризация числовых признаков\n",
    "\n",
    "На графике видно, что цена квартиры имеет максимальную стоимость, если расположены на относительном расстоянии от метро. Если же квартира очень близко или далекто от метро, тогда стоимость начинает падать:\n",
    "\n",
    "<img src=\"img/2_1.png\">\n",
    "\n",
    "Проблема заключается в том, если обучить модель на этих данных, тогда мы не сможем учесть эту зависимость. С уменьшением расстояния до метро цена будет увеличиваться, но это не так, модель не учитывает форму распределения данных. Чтобы это произошло, необходимо применить бинаризацию числовых признаков, а именно, разобъём множество значений признака на бины:\n",
    "\n",
    "<img src=\"img/2_2.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#008B8B\">3. Измерение ошибки в задачах регрессии</h1>\n",
    "\n",
    "### 1) MSE\n",
    "\n",
    "$\\large L(y, a) = (a - y)^2$\n",
    "\n",
    "$\\large \\text{MSE}(a, X) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} (a(x_i) - y_i)^2$\n",
    "\n",
    "### 2) MAE\n",
    "\n",
    "$\\large L(y, a) = |a - y|$\n",
    "\n",
    "$\\large \\text{MAE}(a, X) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} |a(x_i) - y_i|$\n",
    "\n",
    "Предположим, у нас имеются фактические значения объекта $y$ и предсказание модеи $a(x)$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|$$y$$     |$$a(x)$$ |$$|a(x) - y|$$ |$$(a(x)-y)^2$$|\n",
    "|:--------:|:-------:|:-------------:|:------------:|\n",
    "|1         |2        |1              |1             |\n",
    "|1000      |2        |996004         |998           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если изменить веса иодели так, чтобы прогноз сталы на еденицу ближе к ответу для каждого объекта:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|$$y$$     |$$a(x)$$ |$$|a(x) - y|$$ |$$(a(x)-y)^2$$|\n",
    "|:--------:|:-------:|:-------------:|:------------:|\n",
    "|1         |2        |0              |0             |\n",
    "|1000      |3        |994009         |997           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изменив прогноз на еденицу, ошибка для $\\text{MSE}$ на втором объекте уменьшилась на $2000$, а для $\\text{MAE}$ ошибка уменьшилась на еденицу. Это говорит о том, что модель для уменьшения функционала ошибки будет подбирать веса так, чтобы минимизировать выбросы. В случае $\\text{MAE}$ ошибка на двух объекта изменилась только на еденицу, что говорит о устойчивости модели к выбросам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Функция Хубера\n",
    "\n",
    "### 4) Несимметричные функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#008B8B\">4. Обучение линейной регрессии</h1>\n",
    "\n",
    "Заметим, что здесь мы переписали выражение функционала ошибки, заменив $a(x)$ на скалярное призведение $\\left \\langle w,x \\right \\rangle$, после чего мы уже имеем функцию, а не функционал ошибки, так как $Q$ зависит не от некоторой функции $a(x)$, а от вектора весов $w$, и оптимизировать нужно именно по нему, что гораздо проще. Другими словами, в функционал ошибки необходимо подставить только веса, после чего произвести скалярное произведение весов на $x_i$ объект и получить предсказание модели.\n",
    "\n",
    "$\\large Q(w, X) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} (\\langle w, x_i \\rangle - y_i)^2 \\to \\min_{w}$\n",
    "\n",
    "Запишем эту задачу в матричном виде:\n",
    "\n",
    "$\\large X = \\begin{pmatrix}\n",
    "  x_{11} & x_{12} & \\ldots & x_{1d}\\\\\n",
    "  \\ldots & \\ldots & \\ldots & \\ldots\\\\\n",
    "  x_{\\ell 1} & x_{\\ell 2} & \\ldots & x_{\\ell d} \n",
    "\\end{pmatrix}$ - Матрица объекты-признаки\n",
    "\n",
    "$\\large y = \\begin{pmatrix}\n",
    "  y_{1}\\\\\n",
    "  \\ldots\\\\\n",
    "  y_{\\ell} \n",
    "\\end{pmatrix}$\n",
    "$\\large w = \\begin{pmatrix}\n",
    "  w_{1}\\\\\n",
    "  \\ldots\\\\\n",
    "  w_{d} \n",
    "\\end{pmatrix}$\n",
    "\n",
    "Теперь можно записать $\\text{MSE}$ в матричном виде:\n",
    "\n",
    "$\\large Q(w, X) = \\frac{1}{\\ell} \\|X_w - y\\|_2^2 \\to \\min_{w}$\n",
    "\n",
    "$\\large X_w = \\begin{pmatrix}\n",
    "  \\langle w, x_1 \\rangle\\\\\n",
    "  \\ldots\\\\\n",
    "  \\langle w, x_{\\ell} \\rangle \n",
    "\\end{pmatrix}$\n",
    "\n",
    "Применив матричные производные получаем одно решение:\n",
    "\n",
    "$\\large w = (X^T X)^{-1} X^T y$ - (только если $X$ полного ранга)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#008B8B\">5. Градиентный спуск и оценивание градиента</h1>\n",
    "\n",
    "<h2 style=\"color:#008B8B\">5.1 Градиентный спуск</h2>\n",
    "\n",
    "**1)** $\\large w^{(0)}$ - начальное приближение\n",
    "\n",
    "Дальше необходимо сместиться в сторону, куда функция убывает быстрее всего, для этого необходимо найти антиградиент функции в точке с начальным приближением и двигаться в сторону наискорейшего убывания функции, а после произвести замену весов.\n",
    "\n",
    "$\\large \\nabla_w Q(w^{(k-1)}, X)$ - значение градиента функции потерь в точке $w^{(k-1)}$\n",
    "\n",
    "**2)** $\\large w^{(k)} = w^{k-1} - \\eta_k \\nabla_w Q(w^{(k-1)}, X)$ - шаг градиентного спуска\n",
    "\n",
    "**3) Процесс остановки**\n",
    "\n",
    "1. $\\large \\|w^{(k)} - w^{(k-1)} \\| < \\epsilon$ - Если на $k$ шаге вектор весов мало отличается от вектора весов на $k-1$ шаге, тогда производится остановка.\n",
    "\n",
    "2. $\\large \\|Q(w^{(k)}, X) - Q(w^{(k-1)}, X)\\| < \\epsilon$ - Если ошибка на обучении не меняется с предыдущим шагом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4) Сходимость**\n",
    "\n",
    "$\\large \\nabla_w Q(w, X) \\approx 0$ - Гарантируется, что при некоторых условиях градиентый спуск сходится к точке где градиент равен нулю (Это может быть минимум, максимум или седловая точка). \n",
    "\n",
    "Почти все функционалы ошибок для линейных моделей строго выпуклые, значит у данных функционалов одна точка и это точка глобального минимума. Для линейных моделей с матрицей полного ранга точка минимума будет одна и она будет являеться глобальной.\n",
    "\n",
    "Если решений несколько, тогда генерируем несколько начальных приближений и выбираем наименьшюу точку. Данный метод позволяет найти глобальный минимум, если имеются локальные минимумы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#008B8B\">5.2 Оценивание градиента</h2>\n",
    "\n",
    "## Полный градиент\n",
    "Функционал ошибки $\\text{MSE}$ для линейной модели принимает следующий вид:\n",
    "\n",
    "$\\large Q(w, X) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} (\\langle w, x_i \\rangle - y_i)^2$\n",
    "\n",
    "Как правило, в задачах машинного обучения функционал $Q(w, X)$ представим в\n",
    "виде суммы $\\ell$ функций:\n",
    "\n",
    "$\\large Q(w, X) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} q_i(w)$\n",
    "\n",
    "Пример того, чему равняется $q_i(w) = (\\langle w, x_i \\rangle - y_i)^2$.Проблема метода градиентного спуска (5.1) состоит в том, что на каждом шаге\n",
    "необходимо вычислять градиент всей суммы:\n",
    "\n",
    "$\\large \\nabla_w Q(w, X) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} \\nabla_w q_i(w)$ - Полный градиент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стохастический градиентный спуск (SGD)\n",
    "\n",
    "Оценить градиент суммы функций можно градиентом одного случайно взятого\n",
    "слагаемого:\n",
    "\n",
    "$\\large \\nabla_w Q(w) \\approx \\nabla_w q_{i_k}(w)$, где $i_k$ — случайно выбранный номер слагаемого из функционала. В этом случае мы получим метод стохастического градиентного спуска:\n",
    "\n",
    "$\\large w^{(k)} = w^{(k - 1)} - \\eta_k \\nabla q_{i_k}(w^{(k - 1)})$\n",
    "\n",
    "Есть теорема, которая гласит, что если подобрать длину шага так, что ряд длин шагов расходился $\\sum\\limits_{k=1}^{\\infty} \\eta_k = \\infty$, а ряд квадратов длин шагов сходился $\\sum\\limits_{k=1}^{\\infty} \\eta_k^2 < \\infty$, тогда градиент обязательно сойдётся к минимуму. Тогда возникает вопрос, а какую длину шага использовать? (Методичка).\n",
    "\n",
    "**Заметка по скорости сходимости:**\n",
    "Для полного градиента:\n",
    "\n",
    "$\\large Q(w^{(k)}) - Q(w^{*}) = O(\\frac{1}{k})$, где $Q(w^{*})$ -  точка оптимума.\n",
    "\n",
    "Для стохастического градиента:\n",
    "\n",
    "$\\large Q(w^{(k)}) - Q(w^{*}) = O(\\frac{1}{\\sqrt{k}})$\n",
    "\n",
    "В стохастическом градиенте скорость сходимости ниже, что требует больше шагов. Но один шаг в SGD выполняется быстрее, так как не надо каждый раз считать градиент. Поэтому стохастический градиент сходится быстрее полного градиента.\n",
    "\n",
    "**Mini-batch GD**\n",
    "\n",
    "Можно повысить точность оценки градиента, используя несколько слагаемых\n",
    "вместо одного\n",
    "\n",
    "$\\large \\nabla_w Q(w) \\approx \\frac{1}{n} \\sum\\limits_{j = 1}^{n} \\nabla_w q_{i_{kj}}(w)$\n",
    "\n",
    "где $i_{kj}$- случайно выбранные номера слагаемых из функционала ($j$ пробегает значения от $1$ до $n$), а $n$ - параметр метода, размер пачки объектов для одного градиентного шага. С такой оценкой мы получим метод mini-batch gradient descent,\n",
    "который часто используется для обучения дифференцируемых моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
