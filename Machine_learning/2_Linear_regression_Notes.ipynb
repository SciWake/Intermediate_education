{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:black\" align=\"center\">Линейная регрессия</h1>\n",
    "\n",
    "<h1 style=\"color:#008B8B\">1. Линейные модели</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такие модели сводятся к суммированию значений признаков с некоторыми весами:\n",
    "\n",
    "$\\large a(x) = w_0 + \\sum\\limits_{j=1}^d w_j x_j$\n",
    "\n",
    "Параметрами модели являются веса или коэффициенты $w_j$. Вес $w_0$ также называется\n",
    "свободным коэффициентом или сдвигом (bias).\n",
    "\n",
    "Добавим фиктивный признак равный $1$, тогда запись можно упросить до следующего вида:\n",
    "\n",
    "$\\large a(x) = \\sum\\limits_{j=0}^d w_j x_j$\n",
    "\n",
    "$\\large a(x) = \\langle w, x \\rangle$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#008B8B\">2. Область применимости линейных моедлей</h1>\n",
    "\n",
    "Сформулируем задачу: \n",
    "\n",
    "$x$ - кваритра в Москве;\n",
    "\n",
    "$y$ - рычночная стоимость.\n",
    "\n",
    "$\\large a(x) = w_0 + w_1 \\text{(площадь)} + w_2\\text{(количество комнат)} + w_2 \\text{(расстояние до метро)}$\n",
    "\n",
    "Проблема модели заключается в том, что признаки независимо влияют на стоимость квартиры. Если увеличиваетя площадь квартиры, то цена увеличивается только засчёт площади квартиры в линейном виде, при этом растрояние до метро и все остальные признаки не оказывают влияния.\n",
    "Например, если квартира расположена близко от метро, то цена должна увеличиваться быстрее при росте площади."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Категориальные признаки\n",
    "\n",
    "$x_j$ - категориальные признаки\n",
    "\n",
    "$C = \\{c_1, \\ldots, c_m\\}$ - множество значений признака\n",
    "\n",
    "Заменим на $m$ бинарных признаков $b_1(x), \\ldots, b_m(x)$, где $b_i(x) = [x_j = c_i]$.\n",
    "\n",
    "При этом, признаки $b_1(x), \\ldots, b_m(x)$ являются линейно зависимыми: \n",
    "\n",
    "$b_1(x), \\ldots, b_m(x) = 1$\n",
    "\n",
    "И модель принимает следующий вид:\n",
    "\n",
    "$\\large a(x) = w_0 + w_1 [x_j = c_1] + \\ldots + w_m [x_j = c_m] + \\text{\\{Взаимодействие других признаков\\}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Бинаризация числовых признаков\n",
    "\n",
    "На графике видно, что цена квартиры имеет максимальную стоимость, если расположены на относительном расстоянии от метро. Если же квартира очень близко или далекто от метро, тогда стоимость начинает падать:\n",
    "\n",
    "<img src=\"img/2_1.png\">\n",
    "\n",
    "Проблема заключается в том, если обучить модель на этих данных, тогда мы не сможем учесть эту зависимость. С уменьшением расстояния до метро цена будет увеличиваться, но это не так, модель не учитывает форму распределения данных. Чтобы это произошло, необходимо применить бинаризацию числовых признаков, а именно, разобъём множество значений признака на бины:\n",
    "\n",
    "<img src=\"img/2_2.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#008B8B\">3. Измерение ошибки в задачах регрессии</h1>\n",
    "\n",
    "### 1) MSE\n",
    "\n",
    "$\\large L(y, a) = (a - y)^2$\n",
    "\n",
    "$\\large \\text{MSE}(a, X) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} (a(x_i) - y_i)^2$\n",
    "\n",
    "### 2) MAE\n",
    "\n",
    "$\\large L(y, a) = |a - y|$\n",
    "\n",
    "$\\large \\text{MAE}(a, X) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} |a(x_i) - y_i|$\n",
    "\n",
    "Предположим, у нас имеются фактические значения объекта $y$ и предсказание модеи $a(x)$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|$$y$$     |$$a(x)$$ |$$|a(x) - y|$$ |$$(a(x)-y)^2$$|\n",
    "|:--------:|:-------:|:-------------:|:------------:|\n",
    "|1         |2        |1              |1             |\n",
    "|1000      |2        |996004         |998           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если изменить веса иодели так, чтобы прогноз сталы на еденицу ближе к ответу для каждого объекта:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|$$y$$     |$$a(x)$$ |$$|a(x) - y|$$ |$$(a(x)-y)^2$$|\n",
    "|:--------:|:-------:|:-------------:|:------------:|\n",
    "|1         |2        |0              |0             |\n",
    "|1000      |3        |994009         |997           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изменив прогноз на еденицу, ошибка для $\\text{MSE}$ на втором объекте уменьшилась на $2000$, а для $\\text{MAE}$ ошибка уменьшилась на еденицу. Это говорит о том, что модель для уменьшения функционала ошибки будет подбирать веса так, чтобы минимизировать выбросы. В случае $\\text{MAE}$ ошибка на двух объекта изменилась только на еденицу, что говорит о устойчивости модели к выбросам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Средний стохастический градиент (stochastic average gradient)\n",
    "\n",
    "В начале выбираем начальное приближение $w^{(0)}$, после этого необходимо посчитать градиент по все слогаемым из функционала, через $z_i^{(0)}$ обозначим градиенты отдельных слогаемых:\n",
    "\n",
    "$\\large z_i^{(0)} = \\nabla q_i(w^{(0)}), \\qquad i = 1, \\ldots, \\ell$\n",
    "\n",
    "На $k$-й итерации обновляем $z_i$ следующим образом: выбирается случайное слагаемое $i_k$ и для него пересчитываем градиент. А для всех остальных объектов мы переносим оценку градиента с прошлого шага:\n",
    "\n",
    "$\\large z_i^{(k)} = \\begin{cases}\n",
    "    \\nabla q_i(w^{(k - 1)}),\n",
    "    \\quad\n",
    "    &\\text{если}\\ i = i_k;\\\\\n",
    "    z_i^{(k - 1)}\n",
    "    \\quad\n",
    "    & i \\ne i_k.\n",
    "\\end{cases}$\n",
    "\n",
    "Иными словами, пересчитывается один из градиентов слагаемых. Оценка градиента вычисляется как среднее вспомогательных переменных то есть мы используем все слагаемые, как в полном градиенте, но при этом почти все слагаемые берутся с предыдущих шагов, а не пересчитываются:\n",
    "\n",
    "$\\large \\nabla_w Q(w^{(k-1)}, X) \\approx \\frac{1}{\\ell} \\sum\\limits_{i = 1}^{\\ell} z_i^{(k)}$\n",
    "\n",
    "Наконец, делается градиентный шаг:\n",
    "\n",
    "$\\large w^{(k)} = w^{k-1} - \\eta_k \\nabla_w Q(w^{(k-1)}, X)$\n",
    "\n",
    "$\\large w^{(k)} = w^{(k - 1)} - \\eta_k \\frac{1}{\\ell} \\sum\\limits_{i = 1}^{\\ell} z_i^{(k)}$\n",
    "\n",
    "Данный метод имеет такой же порядок сходимости для выпуклых и гладких функционалов,\n",
    "как и обычный градиентный спуск:\n",
    "\n",
    "$\\large Q(w^{(k)}) - Q(w^*) = O(\\frac{1}{k})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#008B8B\">5.3 Модификации градиентного спуска</h2>\n",
    "\n",
    "Вспомним как выглядит шаг \n",
    "\n",
    "$\\large w^{(k)} = w^{k-1} - \\eta_k \\nabla_w Q(w^{(k-1)}, X)$, а на месте $\\nabla_w Q(w^{(k-1)}, X)$ можно использовать оценку градиента заместо полного градиента.\n",
    "\n",
    "### Метод импульса (momentum)\n",
    "\n",
    "**Первая проблема градиентного спуска:**\n",
    "\n",
    "В случае, когда линии уровня вытянуты, тогда градиент будет ортоганален линиям уровня. Но проблема заключается в траектории оптимизации градиента, что тратит много времени.\n",
    "\n",
    "Если построить линии уровня, то мы увидим, что по оси $x$ мы движемся в одном направлении, а по оси $y$ происходят колебания вверх и вниз. Для устранения колебаний и сохранения поступательных движений вдоль оси $x$ необходимо просуммировать вектора и тогда осцилляции по оси $y$ сократят друг друга, а движение по оси $x$ останется.\n",
    "\n",
    "**Реализация**\n",
    "\n",
    "В самом начале, необходимо инициализировать дополнительную переменную нулём:\n",
    "\n",
    "$\\large h_0 = 0$\n",
    "\n",
    "Дальше пересчитаем переменную следующим образом:\n",
    "\n",
    "$\\large h_k = \\alpha h_{k - 1} + \\eta_k \\nabla_w Q(w^{(k-1)})$\n",
    "\n",
    "Здесь $\\alpha$ — параметр метода, определяющей скорость затухания градиентов с предыдущих шагов. Шаг будет выглядеть следующим образом:\n",
    "\n",
    "$\\large w^{(k)} = w^{(k-1)} - h_k$\n",
    "\n",
    "Получается, что в $h_k$ с некоторым затуханием $\\alpha$ мы накапливаем все пердыдущие градиенты, а после уже шагаем в сторону усреднённого градиента."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad и RMSprop\n",
    "\n",
    "**Вторая проблема** (Для стохастического градиента)\n",
    "\n",
    "Пусть имеется разреженный признак $x_j$ - почти на всех объектах нулевой (One-hot encoding). В линейной модели данный признак будет иметь следующий вид:\n",
    "\n",
    "$\\large a(x) = \\ldots + w_j x_j + \\ldots$\n",
    "\n",
    "Если $x_j = 0$, тогда частная производная по $w_j$ тоже будет равна нулю:\n",
    "\n",
    "$\\large x_j = 0 \\Longrightarrow \\frac{\\partial Q}{\\partial w_j} = 0$ А это значит, что на большенстве шагов $w_j$ не будет изменяться. И проблема заключается в том, что $\\eta_k$ убывает по мере роста $k$. На начальных итерациях $\\eta_k$ имеет большое значение, так как мы можем $1000$ итераций не изменять вес $w_j$, а $\\eta_k$ будет убывать, то когда попадается не нулевой признак $w_j$, но длина шаг может быть маленькой, так как мы прошли $1000$ итераций и не разу не изменили $w_j$.\n",
    "\n",
    "Если признаки редко принимают не нулевые значения, тогда необходимо для таких признаков изменять длину шага другим образом.\n",
    "\n",
    "### AdaGrad\n",
    "\n",
    "методе AdaGrad предлагается сделать свою длину шага для каждой компоненты вектора параметров.При этом шаг будет тем меньше, чем более длинные шаги мы делали на предыдущих итерациях:\n",
    "\n",
    "$\\large G_{kj} = G_{k-1,j} + (\\nabla_w Q(w^{(k-1)}))_j^2$ Где $k$ - номер итерации. $j$ - номер признака.\n",
    "\n",
    "$\\large w_j^{(k)} = w_j^{(k-1)} - \\frac{\\eta_k}{\\sqrt{G_{kj} + \\epsilon}} (\\nabla_w Q(w^{(k-1)}))_j$\n",
    "\n",
    "$G_kj$ - Накапливается информация о том, насколько большой шаг по $w_j$ \n",
    "\n",
    "### RMSprop\n",
    "\n",
    "У метода AdaGrad есть большой недостаток: переменная $G_{kj}$ монотонно растёт,\n",
    "из-за чего шаги становятся всё медленнее и могут остановиться ещё до того,\n",
    "как достигнут минимум функционала.\n",
    "Проблема решается в методе RMSprop, где используется экспоненциальное затухание градиентов:\n",
    "\n",
    "$\\large G_{kj} = \\alpha G_{k-1,j} + (1 - \\alpha) (\\nabla_w Q(w^{(k-1)}))_j^2$\n",
    "\n",
    "В этом случае размер шага по координате зависит в основном от того, насколько\n",
    "быстро мы двигались по ней на последних итерациях.\n",
    "\n",
    "### Adam - (Написать) сомешает два метода выше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
