{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:black\" align=\"center\">Линейная регрессия</h1>\n",
    "\n",
    "<h1 style=\"color:#008B8B\">1. Линейные модели</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такие модели сводятся к суммированию значений признаков с некоторыми весами:\n",
    "\n",
    "$\\large a(x) = w_0 + \\sum\\limits_{j=1}^d w_j x_j$\n",
    "\n",
    "Параметрами модели являются веса или коэффициенты $w_j$. Вес $w_0$ также называется\n",
    "свободным коэффициентом или сдвигом (bias).\n",
    "\n",
    "Добавим фиктивный признак равный $1$, тогда запись можно упросить до следующего вида:\n",
    "\n",
    "$\\large a(x) = \\sum\\limits_{j=0}^d w_j x_j$\n",
    "\n",
    "$\\large a(x) = \\langle w, x \\rangle$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#008B8B\">2. Область применимости линейных моедлей</h1>\n",
    "\n",
    "Сформулируем задачу: \n",
    "\n",
    "$x$ - кваритра в Москве;\n",
    "\n",
    "$y$ - рычночная стоимость.\n",
    "\n",
    "$\\large a(x) = w_0 + w_1 \\text{(площадь)} + w_2\\text{(количество комнат)} + w_2 \\text{(расстояние до метро)}$\n",
    "\n",
    "Проблема модели заключается в том, что признаки независимо влияют на стоимость квартиры. Если увеличиваетя площадь квартиры, то цена увеличивается только засчёт площади квартиры в линейном виде, при этом растрояние до метро и все остальные признаки не оказывают влияния.\n",
    "Например, если квартира расположена близко от метро, то цена должна увеличиваться быстрее при росте площади."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Категориальные признаки\n",
    "\n",
    "$x_j$ - категориальные признаки\n",
    "\n",
    "$C = \\{c_1, \\ldots, c_m\\}$ - множество значений признака\n",
    "\n",
    "Заменим на $m$ бинарных признаков $b_1(x), \\ldots, b_m(x)$, где $b_i(x) = [x_j = c_i]$.\n",
    "\n",
    "При этом, признаки $b_1(x), \\ldots, b_m(x)$ являются линейно зависимыми: \n",
    "\n",
    "$b_1(x), \\ldots, b_m(x) = 1$\n",
    "\n",
    "И модель принимает следующий вид:\n",
    "\n",
    "$\\large a(x) = w_0 + w_1 [x_j = c_1] + \\ldots + w_m [x_j = c_m] + \\text{\\{Взаимодействие других признаков\\}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Бинаризация числовых признаков\n",
    "\n",
    "На графике видно, что цена квартиры имеет максимальную стоимость, если расположены на относительном расстоянии от метро. Если же квартира очень близко или далекто от метро, тогда стоимость начинает падать:\n",
    "\n",
    "<img src=\"img/2_1.png\">\n",
    "\n",
    "Проблема заключается в том, если обучить модель на этих данных, тогда мы не сможем учесть эту зависимость. С уменьшением расстояния до метро цена будет увеличиваться, но это не так, модель не учитывает форму распределения данных. Чтобы это произошло, необходимо применить бинаризацию числовых признаков, а именно, разобъём множество значений признака на бины:\n",
    "\n",
    "<img src=\"img/2_2.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#008B8B\">3. Измерение ошибки в задачах регрессии</h1>\n",
    "\n",
    "### 1) MSE\n",
    "\n",
    "$\\large L(y, a) = (a - y)^2$\n",
    "\n",
    "$\\large \\text{MSE}(a, X) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} (a(x_i) - y_i)^2$\n",
    "\n",
    "### 2) MAE\n",
    "\n",
    "$\\large L(y, a) = |a - y|$\n",
    "\n",
    "$\\large \\text{MAE}(a, X) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} |a(x_i) - y_i|$\n",
    "\n",
    "Предположим, у нас имеются фактические значения объекта $y$ и предсказание модеи $a(x)$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|$$y$$     |$$a(x)$$ |$$|a(x) - y|$$ |$$(a(x)-y)^2$$|\n",
    "|:--------:|:-------:|:-------------:|:------------:|\n",
    "|1         |2        |1              |1             |\n",
    "|1000      |2        |996004         |998           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если изменить веса иодели так, чтобы прогноз сталы на еденицу ближе к ответу для каждого объекта:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|$$y$$     |$$a(x)$$ |$$|a(x) - y|$$ |$$(a(x)-y)^2$$|\n",
    "|:--------:|:-------:|:-------------:|:------------:|\n",
    "|1         |2        |0              |0             |\n",
    "|1000      |3        |994009         |997           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изменив прогноз на еденицу, ошибка для $\\text{MSE}$ на втором объекте уменьшилась на $2000$, а для $\\text{MAE}$ ошибка уменьшилась на еденицу. Это говорит о том, что модель для уменьшения функционала ошибки будет подбирать веса так, чтобы минимизировать выбросы. В случае $\\text{MAE}$ ошибка на двух объекта изменилась только на еденицу, что говорит о устойчивости модели к выбросам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Функция Хубера\n",
    "\n",
    "### 4) Несимметричные функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#008B8B\">5. Градиентный спуск и оценивание градиента</h1>\n",
    "\n",
    "<h2 style=\"color:#008B8B\">5.1 Градиентный спуск</h2>\n",
    "\n",
    "**1)** $\\large w^{(0)}$ - начальное приближение\n",
    "\n",
    "Дальше необходимо сместиться в сторону, куда функция убывает быстрее всего, для этого необходимо найти антиградиент функции в точке с начальным приближением и двигаться в сторону наискорейшего убывания функции, а после произвести замену весов.\n",
    "\n",
    "$\\large \\nabla_w Q(w^{(k-1)}, X)$ - значение градиента функции потерь в точке $w^{(k-1)}$\n",
    "\n",
    "**2)** $\\large w^{(k)} = w^{k-1} - \\eta_k \\nabla_w Q(w^{(k-1)}, X)$ - шаг градиентного спуска\n",
    "\n",
    "**3) Процесс остановки**\n",
    "\n",
    "1. $\\large \\|w^{(k)} - w^{(k-1)} \\| < \\epsilon$ - Если на $k$ шаге вектор весов мало отличается от вектора весов на $k-1$ шаге, тогда производится остановка.\n",
    "\n",
    "2. $\\large \\|Q(w^{(k)}, X) - Q(w^{(k-1)}, X)\\| < \\epsilon$ - Если ошибка на обучении не меняется с предыдущим шагом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4) Сходимость**\n",
    "\n",
    "$\\large \\nabla_w Q(w, X) \\approx 0$ - Гарантируется, что при некоторых условиях градиентый спуск сходится к точке где градиент равен нулю (Это может быть минимум, максимум или седловая точка). \n",
    "\n",
    "Почти все функционалы ошибок для линейных моделей строго выпуклые, значит у данных функционалов одна точка и это точка глобального минимума. Для линейных моделей с матрицей полного ранга точка минимума будет одна и она будет являеться глобальной.\n",
    "\n",
    "Если решений несколько, тогда генерируем несколько начальных приближений и выбираем наименьшюу точку. Данный метод позволяет найти глобальный минимум, если имеются локальные минимумы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#008B8B\">5.2 Оценивание градиента</h2>\n",
    "\n",
    "## Полный градиент\n",
    "Функционал ошибки $\\text{MSE}$ для линейной модели принимает следующий вид:\n",
    "\n",
    "$\\large Q(w, X) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} (\\langle w, x_i \\rangle - y_i)^2$\n",
    "\n",
    "Как правило, в задачах машинного обучения функционал $Q(w, X)$ представим в\n",
    "виде суммы $\\ell$ функций:\n",
    "\n",
    "$\\large Q(w, X) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} q_i(w)$\n",
    "\n",
    "Пример того, чему равняется $q_i(w) = (\\langle w, x_i \\rangle - y_i)^2$.Проблема метода градиентного спуска (5.1) состоит в том, что на каждом шаге\n",
    "необходимо вычислять градиент всей суммы:\n",
    "\n",
    "$\\large \\nabla_w Q(w, X) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} \\nabla_w q_i(w)$ - Полный градиент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стохастический градиентный спуск (SGD)\n",
    "\n",
    "Оценить градиент суммы функций можно градиентом одного случайно взятого\n",
    "слагаемого:\n",
    "\n",
    "$\\large \\nabla_w Q(w, X) \\approx \\nabla_w q_{i_k}(w)$, где $i_k$ — случайно выбранный номер слагаемого из функционала. В этом случае мы получим метод стохастического градиентного спуска:\n",
    "\n",
    "$\\large w^{(k)} = w^{(k - 1)} - \\eta_k \\nabla q_{i_k}(w^{(k - 1)})$\n",
    "\n",
    "Есть теорема, которая гласит, что если подобрать длину шага так, что ряд длин шагов расходился $\\sum\\limits_{k=1}^{\\infty} \\eta_k = \\infty$, а ряд квадратов длин шагов сходился $\\sum\\limits_{k=1}^{\\infty} \\eta_k^2 < \\infty$, тогда градиент обязательно сойдётся к минимуму. Тогда возникает вопрос, а какую длину шага использовать? (Методичка).\n",
    "\n",
    "**Заметка по скорости сходимости:**\n",
    "Для полного градиента:\n",
    "\n",
    "$\\large Q(w^{(k)}) - Q(w^{*}) = O(\\frac{1}{k})$, где $Q(w^{*})$ -  точка оптимума.\n",
    "\n",
    "Для стохастического градиента:\n",
    "\n",
    "$\\large Q(w^{(k)}) - Q(w^{*}) = O(\\frac{1}{\\sqrt{k}})$\n",
    "\n",
    "В стохастическом градиенте скорость сходимости ниже, что требует больше шагов. Но один шаг в SGD выполняется быстрее, так как не надо каждый раз считать градиент. Поэтому стохастический градиент сходится быстрее полного градиента.\n",
    "\n",
    "**Mini-batch GD**\n",
    "\n",
    "Можно повысить точность оценки градиента, используя несколько слагаемых\n",
    "вместо одного\n",
    "\n",
    "$\\large \\nabla_w Q(w, X) \\approx \\frac{1}{n} \\sum\\limits_{j = 1}^{n} \\nabla_w q_{i_{kj}}(w)$\n",
    "\n",
    "где $i_{kj}$- случайно выбранные номера слагаемых из функционала ($j$ пробегает значения от $1$ до $n$), а $n$ - параметр метода, размер пачки объектов для одного градиентного шага. С такой оценкой мы получим метод mini-batch gradient descent,\n",
    "который часто используется для обучения дифференцируемых моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Средний стохастический градиент (stochastic average gradient)\n",
    "\n",
    "В начале выбираем начальное приближение $w^{(0)}$, после этого необходимо посчитать градиент по все слогаемым из функционала, через $z_i^{(0)}$ обозначим градиенты отдельных слогаемых:\n",
    "\n",
    "$\\large z_i^{(0)} = \\nabla q_i(w^{(0)}), \\qquad i = 1, \\ldots, \\ell$\n",
    "\n",
    "На $k$-й итерации обновляем $z_i$ следующим образом: выбирается случайное слагаемое $i_k$ и для него пересчитываем градиент. А для всех остальных объектов мы переносим оценку градиента с прошлого шага:\n",
    "\n",
    "$\\large z_i^{(k)} = \\begin{cases}\n",
    "    \\nabla q_i(w^{(k - 1)}),\n",
    "    \\quad\n",
    "    &\\text{если}\\ i = i_k;\\\\\n",
    "    z_i^{(k - 1)}\n",
    "    \\quad\n",
    "    & i \\ne i_k.\n",
    "\\end{cases}$\n",
    "\n",
    "Иными словами, пересчитывается один из градиентов слагаемых. Оценка градиента вычисляется как среднее вспомогательных переменных то есть мы используем все слагаемые, как в полном градиенте, но при этом почти все слагаемые берутся с предыдущих шагов, а не пересчитываются:\n",
    "\n",
    "$\\large \\nabla_w Q(w^{(k-1)}, X) \\approx \\frac{1}{\\ell} \\sum\\limits_{i = 1}^{\\ell} z_i^{(k)}$\n",
    "\n",
    "Наконец, делается градиентный шаг:\n",
    "\n",
    "$\\large w^{(k)} = w^{k-1} - \\eta_k \\nabla_w Q(w^{(k-1)}, X)$\n",
    "\n",
    "$\\large w^{(k)} = w^{(k - 1)} - \\eta_k \\frac{1}{\\ell} \\sum\\limits_{i = 1}^{\\ell} z_i^{(k)}$\n",
    "\n",
    "Данный метод имеет такой же порядок сходимости для выпуклых и гладких функционалов,\n",
    "как и обычный градиентный спуск:\n",
    "\n",
    "$\\large Q(w^{(k)}) - Q(w^*) = O(\\frac{1}{k})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#008B8B\">5.3 Модификации градиентного спуска</h2>\n",
    "\n",
    "Вспомним как выглядит шаг \n",
    "\n",
    "$\\large w^{(k)} = w^{k-1} - \\eta_k \\nabla_w Q(w^{(k-1)}, X)$, а на месте $\\nabla_w Q(w^{(k-1)}, X)$ можно использовать оценку градиента заместо полного градиента.\n",
    "\n",
    "### Метод импульса (momentum)\n",
    "\n",
    "**Первая проблема градиентного спуска:**\n",
    "\n",
    "В случае, когда линии уровня вытянуты, тогда градиент будет ортоганален линиям уровня. Но проблема заключается в траектории оптимизации градиента, что тратит много времени.\n",
    "\n",
    "Если построить линии уровня, то мы увидим, что по оси $x$ мы движемся в одном направлении, а по оси $y$ происходят колебания вверх и вниз. Для устранения колебаний и сохранения поступательных движений вдоль оси $x$ необходимо просуммировать вектора и тогда осцилляции по оси $y$ сократят друг друга, а движение по оси $x$ останется.\n",
    "\n",
    "**Реализация**\n",
    "\n",
    "В самом начале, необходимо инициализировать дополнительную переменную нулём:\n",
    "\n",
    "$\\large h_0 = 0$\n",
    "\n",
    "Дальше пересчитаем переменную следующим образом:\n",
    "\n",
    "$\\large h_k = \\alpha h_{k - 1} + \\eta_k \\nabla_w Q(w^{(k-1)})$\n",
    "\n",
    "Здесь $\\alpha$ — параметр метода, определяющей скорость затухания градиентов с предыдущих шагов. Шаг будет выглядеть следующим образом:\n",
    "\n",
    "$\\large w^{(k)} = w^{(k-1)} - h_k$\n",
    "\n",
    "Получается, что в $h_k$ с некоторым затуханием $\\alpha$ мы накапливаем все пердыдущие градиенты, а после уже шагаем в сторону усреднённого градиента."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad и RMSprop\n",
    "\n",
    "**Вторая проблема** (Для стохастического градиента)\n",
    "\n",
    "Пусть имеется разреженный признак $x_j$ - почти на всех объектах нулевой (One-hot encoding). В линейной модели данный признак будет иметь следующий вид:\n",
    "\n",
    "$\\large a(x) = \\ldots + w_j x_j + \\ldots$\n",
    "\n",
    "Если $x_j = 0$, тогда частная производная по $w_j$ тоже будет равна нулю:\n",
    "\n",
    "$\\large x_j = 0 \\Longrightarrow \\frac{\\partial Q}{\\partial w_j} = 0$ А это значит, что на большенстве шагов $w_j$ не будет изменяться. И проблема заключается в том, что $\\eta_k$ убывает по мере роста $k$. На начальных итерациях $\\eta_k$ имеет большое значение, так как мы можем $1000$ итераций не изменять вес $w_j$, а $\\eta_k$ будет убывать, то когда попадается не нулевой признак $w_j$, но длина шаг может быть маленькой, так как мы прошли $1000$ итераций и не разу не изменили $w_j$.\n",
    "\n",
    "Если признаки редко принимают не нулевые значения, тогда необходимо для таких признаков изменять длину шага другим образом.\n",
    "\n",
    "### AdaGrad\n",
    "\n",
    "методе AdaGrad предлагается сделать свою длину шага для каждой компоненты вектора параметров.При этом шаг будет тем меньше, чем более длинные шаги мы делали на предыдущих итерациях:\n",
    "\n",
    "$\\large G_{kj} = G_{k-1,j} + (\\nabla_w Q(w^{(k-1)}))_j^2$ Где $k$ - номер итерации. $j$ - номер признака.\n",
    "\n",
    "$\\large w_j^{(k)} = w_j^{(k-1)} - \\frac{\\eta_k}{\\sqrt{G_{kj} + \\epsilon}} (\\nabla_w Q(w^{(k-1)}))_j$\n",
    "\n",
    "$G_kj$ - Накапливается информация о том, насколько большой шаг по $w_j$ \n",
    "\n",
    "### RMSprop\n",
    "\n",
    "У метода AdaGrad есть большой недостаток: переменная $G_{kj}$ монотонно растёт,\n",
    "из-за чего шаги становятся всё медленнее и могут остановиться ещё до того,\n",
    "как достигнут минимум функционала.\n",
    "Проблема решается в методе RMSprop, где используется экспоненциальное затухание градиентов:\n",
    "\n",
    "$\\large G_{kj} = \\alpha G_{k-1,j} + (1 - \\alpha) (\\nabla_w Q(w^{(k-1)}))_j^2$\n",
    "\n",
    "В этом случае размер шага по координате зависит в основном от того, насколько\n",
    "быстро мы двигались по ней на последних итерациях.\n",
    "\n",
    "### Adam - (Написать) сомешает два метода выше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
