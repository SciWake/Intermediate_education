{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09d78cc0",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#008B8B\">3. Обучение линейной регрессии</h1>\n",
    "\n",
    "Заметим, что здесь мы переписали выражение функционала ошибки, заменив $a(x)$ на скалярное призведение $\\left \\langle w,x \\right \\rangle$, после чего мы уже имеем функцию, а не функционал ошибки, так как $Q$ зависит не от некоторой функции $a(x)$, а от вектора весов $w$, и оптимизировать нужно именно по нему, что гораздо проще. Другими словами, в функционал ошибки необходимо подставить только веса, после чего произвести скалярное произведение весов на $x_i$ объект и получить предсказание модели.\n",
    "\n",
    "$\\large Q(w, X) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} (\\langle w, x_i \\rangle - y_i)^2 \\to \\underset{w}{\\text{min}}$\n",
    "\n",
    "Запишем эту задачу в матричном виде:\n",
    "\n",
    "$\\large X = \\begin{pmatrix}\n",
    "  x_{11} & x_{12} & \\ldots & x_{1d}\\\\\n",
    "  \\ldots & \\ldots & \\ldots & \\ldots\\\\\n",
    "  x_{\\ell 1} & x_{\\ell 2} & \\ldots & x_{\\ell d} \n",
    "\\end{pmatrix}$ - Матрица объекты-признаки\n",
    "\n",
    "$\\large y = \\begin{pmatrix}\n",
    "  y_{1}\\\\\n",
    "  \\ldots\\\\\n",
    "  y_{\\ell} \n",
    "\\end{pmatrix}$\n",
    "$\\large w = \\begin{pmatrix}\n",
    "  w_{1}\\\\\n",
    "  \\ldots\\\\\n",
    "  w_{d} \n",
    "\\end{pmatrix}$\n",
    "\n",
    "Теперь можно записать $\\text{MSE}$ в матричном виде:\n",
    "\n",
    "$\\large Q(w, X) = \\frac{1}{\\ell} \\|Xw - y\\|_2^2 \\to \\underset{w}{\\text{min}}$\n",
    "\n",
    "\n",
    "$\\large X_w = \\begin{pmatrix}\n",
    "  \\langle w, x_1 \\rangle\\\\\n",
    "  \\ldots\\\\\n",
    "  \\langle w, x_{\\ell} \\rangle \n",
    "\\end{pmatrix}$\n",
    "\n",
    "Найдём градиент:\n",
    "\n",
    "$\\large \\nabla_{w}Q(w,X) = \\frac{2}{\\ell}X^{T}(Xw-y)$\n",
    "\n",
    "Приравняем выражение градиента к $0$ для получения аналитического решения:\n",
    "\n",
    "$\\large \\frac{2}{\\ell}X^{T}(Xw-y) = 0$\n",
    "\n",
    "$\\large X^{T}(Xw-y) = 0$\n",
    "\n",
    "$\\large X^{T}Xw-X^{T}y = 0$\n",
    "\n",
    "$\\large w = \\frac{X^{T}y}{X^{T}X}$\n",
    "\n",
    "$\\large w^* = (X^T X)^{-1} X^T y$ - (только если $X$ полного ранга)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881b1ff1",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#008B8B\">4. Градиентный спуск и оценивание градиента</h1>\n",
    "\n",
    "<h2 style=\"color:#008B8B\">4.1 Градиентный спуск</h2>\n",
    "\n",
    "**1)** $\\large w^{(0)}$ - начальное приближение\n",
    "\n",
    "Дальше необходимо сместиться в сторону, куда функция убывает быстрее всего, для этого необходимо найти антиградиент функции в точке с начальным приближением и двигаться в сторону наискорейшего убывания функции, а после произвести замену весов.\n",
    "\n",
    "$\\large \\nabla_w Q(w^{(k-1)}, X)$ - значение градиента функции потерь в точке $w^{(k-1)}$\n",
    "\n",
    "**2)** $\\large w^{(k)} = w^{k-1} - \\eta_k \\nabla_w Q(w^{(k-1)}, X)$ - шаг градиентного спуска\n",
    "\n",
    "\n",
    "**3) Процесс остановки**\n",
    "\n",
    "1. $\\large \\|w^{(k)} - w^{(k-1)} \\| < \\epsilon$ - Если на $k$ шаге вектор весов мало отличается от вектора весов на $k-1$ шаге, тогда производится остановка.\n",
    "\n",
    "2. $\\large \\|Q(w^{(k)}, X) - Q(w^{(k-1)}, X)\\| < \\epsilon$ - Если ошибка на обучении не меняется с предыдущим шагом\n",
    "\n",
    "\n",
    "**4) Сходимость**\n",
    "\n",
    "$\\large \\nabla_w Q(w, X) \\approx 0$ - Гарантируется, что при некоторых условиях градиентый спуск сходится к точке где градиент равен нулю (Это может быть минимум, максимум или седловая точка). \n",
    "\n",
    "Почти все функционалы ошибок для линейных моделей строго выпуклые, значит у данных функционалов одна точка и это точка глобального минимума. Для линейных моделей с матрицей полного ранга точка минимума будет одна и она будет являеться глобальной.\n",
    "\n",
    "Если решений несколько, тогда генерируем несколько начальных приближений и выбираем наименьшюу точку. Данный метод позволяет найти глобальный минимум, если имеются локальные минимумы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46227e4",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#008B8B\">4.2 Оценивание градиента</h2>\n",
    "\n",
    "## Полный градиент\n",
    "Функционал ошибки $\\text{MSE}$ для линейной модели принимает следующий вид:\n",
    "\n",
    "$\\large Q(w, X) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} (\\langle w, x_i \\rangle - y_i)^2$\n",
    "\n",
    "Как правило, в задачах машинного обучения функционал $Q(w, X)$ представим в\n",
    "виде суммы $\\ell$ функций:\n",
    "\n",
    "$\\large Q(w, X) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} q_i(w)$\n",
    "\n",
    "Пример того, чему равняется $q_i(w) = (\\langle w, x_i \\rangle - y_i)^2$.Проблема метода градиентного спуска (5.1) состоит в том, что на каждом шаге\n",
    "необходимо вычислять градиент всей суммы:\n",
    "\n",
    "$\\large \\nabla_w Q(w, X) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} \\nabla_w q_i(w)$ - Полный градиент"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb356488",
   "metadata": {},
   "source": [
    "## Стохастический градиентный спуск (SGD)\n",
    "\n",
    "Оценить градиент суммы функций можно градиентом одного случайно взятого\n",
    "слагаемого:\n",
    "\n",
    "$\\large \\nabla_w Q(w, X) \\approx \\nabla_w q_{i_k}(w)$, где $i_k$ — случайно выбранный номер слагаемого из функционала. В этом случае мы получим метод стохастического градиентного спуска:\n",
    "\n",
    "$\\large w^{(k)} = w^{(k - 1)} - \\eta_k \\nabla q_{i_k}(w^{(k - 1)})$\n",
    "\n",
    "Есть теорема, которая гласит, что если подобрать длину шага так, что ряд длин шагов расходился $\\sum\\limits_{k=1}^{\\infty} \\eta_k = \\infty$, а ряд квадратов длин шагов сходился $\\sum\\limits_{k=1}^{\\infty} \\eta_k^2 < \\infty$, тогда градиент обязательно сойдётся к минимуму. Тогда возникает вопрос, а какую длину шага использовать? (Методичка).\n",
    "\n",
    "**Заметка по скорости сходимости:**\n",
    "Для полного градиента:\n",
    "\n",
    "$\\large Q(w^{(k)}) - Q(w^{*}) = O(\\frac{1}{k})$, где $Q(w^{*})$ -  точка оптимума.\n",
    "\n",
    "Для стохастического градиента:\n",
    "\n",
    "$\\large Q(w^{(k)}) - Q(w^{*}) = O(\\frac{1}{\\sqrt{k}})$\n",
    "\n",
    "В стохастическом градиенте скорость сходимости ниже, что требует больше шагов. Но один шаг в SGD выполняется быстрее, так как не надо каждый раз считать градиент. Поэтому стохастический градиент сходится быстрее полного градиента.\n",
    "\n",
    "**Mini-batch GD**\n",
    "\n",
    "Можно повысить точность оценки градиента, используя несколько слагаемых\n",
    "вместо одного\n",
    "\n",
    "$\\large \\nabla_w Q(w, X) \\approx \\frac{1}{n} \\sum\\limits_{j = 1}^{n} \\nabla_w q_{i_{kj}}(w)$\n",
    "\n",
    "где $i_{kj}$- случайно выбранные номера слагаемых из функционала ($j$ пробегает значения от $1$ до $n$), а $n$ - параметр метода, размер пачки объектов для одного градиентного шага. С такой оценкой мы получим метод mini-batch gradient descent,\n",
    "который часто используется для обучения дифференцируемых моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a7cb4c",
   "metadata": {},
   "source": [
    "## Средний стохастический градиент (stochastic average gradient)\n",
    "\n",
    "В начале выбираем начальное приближение $w^{(0)}$, после этого необходимо посчитать градиент по все слогаемым из функционала, через $z_i^{(0)}$ обозначим градиенты отдельных слогаемых:\n",
    "\n",
    "$\\large z_i^{(0)} = \\nabla q_i(w^{(0)}), \\qquad i = 1, \\ldots, \\ell$\n",
    "\n",
    "На $k$-й итерации обновляем $z_i$ следующим образом: выбирается случайное слагаемое $i_k$ и для него пересчитываем градиент. А для всех остальных объектов мы переносим оценку градиента с прошлого шага:\n",
    "\n",
    "$\\large z_i^{(k)} = \\begin{cases}\n",
    "    \\nabla q_i(w^{(k - 1)}),\n",
    "    \\quad\n",
    "    &\\text{если}\\ i = i_k;\\\\\n",
    "    z_i^{(k - 1)}\n",
    "    \\quad\n",
    "    & i \\ne i_k.\n",
    "\\end{cases}$\n",
    "\n",
    "Иными словами, пересчитывается один из градиентов слагаемых. Оценка градиента вычисляется как среднее вспомогательных переменных то есть мы используем все слагаемые, как в полном градиенте, но при этом почти все слагаемые берутся с предыдущих шагов, а не пересчитываются:\n",
    "\n",
    "$\\large \\nabla_w Q(w^{(k-1)}, X) \\approx \\frac{1}{\\ell} \\sum\\limits_{i = 1}^{\\ell} z_i^{(k)}$\n",
    "\n",
    "Наконец, делается градиентный шаг:\n",
    "\n",
    "$\\large w^{(k)} = w^{k-1} - \\eta_k \\nabla_w Q(w^{(k-1)}, X)$\n",
    "\n",
    "$\\large w^{(k)} = w^{(k - 1)} - \\eta_k \\frac{1}{\\ell} \\sum\\limits_{i = 1}^{\\ell} z_i^{(k)}$\n",
    "\n",
    "Данный метод имеет такой же порядок сходимости для выпуклых и гладких функционалов,\n",
    "как и обычный градиентный спуск:\n",
    "\n",
    "$\\large Q(w^{(k)}) - Q(w^*) = O(\\frac{1}{k})$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a44b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
