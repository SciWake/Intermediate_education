{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b80227f7",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#008B8B\">3. Обучение линейной регрессии</h1>\n",
    "\n",
    "Заметим, что здесь мы переписали выражение функционала ошибки, заменив $a(x)$ на скалярное призведение $\\left \\langle w,x \\right \\rangle$, после чего мы уже имеем функцию, а не функционал ошибки, так как $Q$ зависит не от некоторой функции $a(x)$, а от вектора весов $w$, и оптимизировать нужно именно по нему, что гораздо проще. Другими словами, в функционал ошибки необходимо подставить только веса, после чего произвести скалярное произведение весов на $x_i$ объект и получить предсказание модели.\n",
    "\n",
    "$\\large Q(w, X) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} (\\langle w, x_i \\rangle - y_i)^2 \\to \\underset{w}{\\text{min}}$\n",
    "\n",
    "Запишем эту задачу в матричном виде:\n",
    "\n",
    "$\\large X = \\begin{pmatrix}\n",
    "  x_{11} & x_{12} & \\ldots & x_{1d}\\\\\n",
    "  \\ldots & \\ldots & \\ldots & \\ldots\\\\\n",
    "  x_{\\ell 1} & x_{\\ell 2} & \\ldots & x_{\\ell d} \n",
    "\\end{pmatrix}$ - Матрица объекты-признаки\n",
    "\n",
    "$\\large y = \\begin{pmatrix}\n",
    "  y_{1}\\\\\n",
    "  \\ldots\\\\\n",
    "  y_{\\ell} \n",
    "\\end{pmatrix}$\n",
    "$\\large w = \\begin{pmatrix}\n",
    "  w_{1}\\\\\n",
    "  \\ldots\\\\\n",
    "  w_{d} \n",
    "\\end{pmatrix}$\n",
    "\n",
    "Теперь можно записать $\\text{MSE}$ в матричном виде:\n",
    "\n",
    "$\\large Q(w, X) = \\frac{1}{\\ell} \\|Xw - y\\|_2^2 \\to \\underset{w}{\\text{min}}$\n",
    "\n",
    "\n",
    "$\\large X_w = \\begin{pmatrix}\n",
    "  \\langle w, x_1 \\rangle\\\\\n",
    "  \\ldots\\\\\n",
    "  \\langle w, x_{\\ell} \\rangle \n",
    "\\end{pmatrix}$\n",
    "\n",
    "Найдём градиент:\n",
    "\n",
    "$\\large \\nabla_{w}Q(w,X) = \\frac{2}{\\ell}X^{T}(Xw-y)$\n",
    "\n",
    "Приравняем выражение градиента к $0$ для получения аналитического решения:\n",
    "\n",
    "$\\large \\frac{2}{\\ell}X^{T}(Xw-y) = 0$\n",
    "\n",
    "$\\large X^{T}(Xw-y) = 0$\n",
    "\n",
    "$\\large X^{T}Xw-X^{T}y = 0$\n",
    "\n",
    "$\\large w = \\frac{X^{T}y}{X^{T}X}$\n",
    "\n",
    "$\\large w^* = (X^T X)^{-1} X^T y$ - (только если $X$ полного ранга)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93234abe",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#008B8B\">4. Градиентный спуск и оценивание градиента</h1>\n",
    "\n",
    "<h2 style=\"color:#008B8B\">4.1 Градиентный спуск</h2>\n",
    "\n",
    "**1)** $\\large w^{(0)}$ - начальное приближение\n",
    "\n",
    "Дальше необходимо сместиться в сторону, куда функция убывает быстрее всего, для этого необходимо найти антиградиент функции в точке с начальным приближением и двигаться в сторону наискорейшего убывания функции, а после произвести замену весов.\n",
    "\n",
    "$\\large \\nabla_w Q(w^{(k-1)}, X)$ - значение градиента функции потерь в точке $w^{(k-1)}$\n",
    "\n",
    "**2)** $\\large w^{(k)} = w^{k-1} - \\eta_k \\nabla_w Q(w^{(k-1)}, X)$ - шаг градиентного спуска\n",
    "\n",
    "\n",
    "**3) Процесс остановки**\n",
    "\n",
    "1. $\\large \\|w^{(k)} - w^{(k-1)} \\| < \\epsilon$ - Если на $k$ шаге вектор весов мало отличается от вектора весов на $k-1$ шаге, тогда производится остановка.\n",
    "\n",
    "2. $\\large \\|Q(w^{(k)}, X) - Q(w^{(k-1)}, X)\\| < \\epsilon$ - Если ошибка на обучении не меняется с предыдущим шагом\n",
    "\n",
    "\n",
    "**4) Сходимость**\n",
    "\n",
    "$\\large \\nabla_w Q(w, X) \\approx 0$ - Гарантируется, что при некоторых условиях градиентый спуск сходится к точке где градиент равен нулю (Это может быть минимум, максимум или седловая точка). \n",
    "\n",
    "Почти все функционалы ошибок для линейных моделей строго выпуклые, значит у данных функционалов одна точка и это точка глобального минимума. Для линейных моделей с матрицей полного ранга точка минимума будет одна и она будет являеться глобальной.\n",
    "\n",
    "Если решений несколько, тогда генерируем несколько начальных приближений и выбираем наименьшюу точку. Данный метод позволяет найти глобальный минимум, если имеются локальные минимумы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7ac994",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#008B8B\">4.2 Оценивание градиента</h2>\n",
    "\n",
    "## Полный градиент\n",
    "Функционал ошибки $\\text{MSE}$ для линейной модели принимает следующий вид:\n",
    "\n",
    "$\\large Q(w, X) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} (\\langle w, x_i \\rangle - y_i)^2$\n",
    "\n",
    "Как правило, в задачах машинного обучения функционал $Q(w, X)$ представим в\n",
    "виде суммы $\\ell$ функций:\n",
    "\n",
    "$\\large Q(w, X) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} q_i(w)$\n",
    "\n",
    "Пример того, чему равняется $q_i(w) = (\\langle w, x_i \\rangle - y_i)^2$.Проблема метода градиентного спуска (5.1) состоит в том, что на каждом шаге\n",
    "необходимо вычислять градиент всей суммы:\n",
    "\n",
    "$\\large \\nabla_w Q(w, X) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} \\nabla_w q_i(w)$ - Полный градиент"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd50fc9",
   "metadata": {},
   "source": [
    "## Стохастический градиентный спуск (SGD)\n",
    "\n",
    "Оценить градиент суммы функций можно градиентом одного случайно взятого\n",
    "слагаемого:\n",
    "\n",
    "$\\large \\nabla_w Q(w, X) \\approx \\nabla_w q_{i_k}(w)$, где $i_k$ — случайно выбранный номер слагаемого из функционала. В этом случае мы получим метод стохастического градиентного спуска:\n",
    "\n",
    "$\\large w^{(k)} = w^{(k - 1)} - \\eta_k \\nabla q_{i_k}(w^{(k - 1)})$\n",
    "\n",
    "Есть теорема, которая гласит, что если подобрать длину шага так, что ряд длин шагов расходился $\\sum\\limits_{k=1}^{\\infty} \\eta_k = \\infty$, а ряд квадратов длин шагов сходился $\\sum\\limits_{k=1}^{\\infty} \\eta_k^2 < \\infty$, тогда градиент обязательно сойдётся к минимуму. Тогда возникает вопрос, а какую длину шага использовать? (Методичка).\n",
    "\n",
    "**Заметка по скорости сходимости:**\n",
    "Для полного градиента:\n",
    "\n",
    "$\\large Q(w^{(k)}) - Q(w^{*}) = O(\\frac{1}{k})$, где $Q(w^{*})$ -  точка оптимума.\n",
    "\n",
    "Для стохастического градиента:\n",
    "\n",
    "$\\large Q(w^{(k)}) - Q(w^{*}) = O(\\frac{1}{\\sqrt{k}})$\n",
    "\n",
    "В стохастическом градиенте скорость сходимости ниже, что требует больше шагов. Но один шаг в SGD выполняется быстрее, так как не надо каждый раз считать градиент. Поэтому стохастический градиент сходится быстрее полного градиента.\n",
    "\n",
    "**Mini-batch GD**\n",
    "\n",
    "Можно повысить точность оценки градиента, используя несколько слагаемых\n",
    "вместо одного\n",
    "\n",
    "$\\large \\nabla_w Q(w, X) \\approx \\frac{1}{n} \\sum\\limits_{j = 1}^{n} \\nabla_w q_{i_{kj}}(w)$\n",
    "\n",
    "где $i_{kj}$- случайно выбранные номера слагаемых из функционала ($j$ пробегает значения от $1$ до $n$), а $n$ - параметр метода, размер пачки объектов для одного градиентного шага. С такой оценкой мы получим метод mini-batch gradient descent,\n",
    "который часто используется для обучения дифференцируемых моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a01092c",
   "metadata": {},
   "source": [
    "## Средний стохастический градиент (stochastic average gradient)\n",
    "\n",
    "В начале выбираем начальное приближение $w^{(0)}$, после этого необходимо посчитать градиент по все слогаемым из функционала, через $z_i^{(0)}$ обозначим градиенты отдельных слогаемых:\n",
    "\n",
    "$\\large z_i^{(0)} = \\nabla q_i(w^{(0)}), \\qquad i = 1, \\ldots, \\ell$\n",
    "\n",
    "На $k$-й итерации обновляем $z_i$ следующим образом: выбирается случайное слагаемое $i_k$ и для него пересчитываем градиент. А для всех остальных объектов мы переносим оценку градиента с прошлого шага:\n",
    "\n",
    "$\\large z_i^{(k)} = \\begin{cases}\n",
    "    \\nabla q_i(w^{(k - 1)}),\n",
    "    \\quad\n",
    "    &\\text{если}\\ i = i_k;\\\\\n",
    "    z_i^{(k - 1)}\n",
    "    \\quad\n",
    "    & i \\ne i_k.\n",
    "\\end{cases}$\n",
    "\n",
    "Иными словами, пересчитывается один из градиентов слагаемых. Оценка градиента вычисляется как среднее вспомогательных переменных то есть мы используем все слагаемые, как в полном градиенте, но при этом почти все слагаемые берутся с предыдущих шагов, а не пересчитываются:\n",
    "\n",
    "$\\large \\nabla_w Q(w^{(k-1)}, X) \\approx \\frac{1}{\\ell} \\sum\\limits_{i = 1}^{\\ell} z_i^{(k)}$\n",
    "\n",
    "Наконец, делается градиентный шаг:\n",
    "\n",
    "$\\large w^{(k)} = w^{k-1} - \\eta_k \\nabla_w Q(w^{(k-1)}, X)$\n",
    "\n",
    "$\\large w^{(k)} = w^{(k - 1)} - \\eta_k \\frac{1}{\\ell} \\sum\\limits_{i = 1}^{\\ell} z_i^{(k)}$\n",
    "\n",
    "Данный метод имеет такой же порядок сходимости для выпуклых и гладких функционалов,\n",
    "как и обычный градиентный спуск:\n",
    "\n",
    "$\\large Q(w^{(k)}) - Q(w^*) = O(\\frac{1}{k})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb1770c",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#008B8B\">5.3 Модификации градиентного спуска</h2>\n",
    "\n",
    "Вспомним как выглядит шаг \n",
    "\n",
    "$\\large w^{(k)} = w^{k-1} - \\eta_k \\nabla_w Q(w^{(k-1)}, X)$, а на месте $\\nabla_w Q(w^{(k-1)}, X)$ можно использовать оценку градиента заместо полного градиента.\n",
    "\n",
    "### Метод импульса (momentum)\n",
    "\n",
    "**Первая проблема градиентного спуска:**\n",
    "\n",
    "В случае, когда линии уровня вытянуты, тогда градиент будет ортоганален линиям уровня. Но проблема заключается в траектории оптимизации градиента, что тратит много времени.\n",
    "\n",
    "Если построить линии уровня, то мы увидим, что по оси $x$ мы движемся в одном направлении, а по оси $y$ происходят колебания вверх и вниз. Для устранения колебаний и сохранения поступательных движений вдоль оси $x$ необходимо просуммировать вектора и тогда осцилляции по оси $y$ сократят друг друга, а движение по оси $x$ останется.\n",
    "\n",
    "**Реализация**\n",
    "\n",
    "В самом начале, необходимо инициализировать дополнительную переменную нулём:\n",
    "\n",
    "$\\large h_0 = 0$\n",
    "\n",
    "Дальше пересчитаем переменную следующим образом:\n",
    "\n",
    "$\\large h_k = \\alpha h_{k - 1} + \\eta_k \\nabla_w Q(w^{(k-1)})$\n",
    "\n",
    "Здесь $\\alpha$ — параметр метода, определяющей скорость затухания градиентов с предыдущих шагов. Шаг будет выглядеть следующим образом:\n",
    "\n",
    "$\\large w^{(k)} = w^{(k-1)} - h_k$\n",
    "\n",
    "Получается, что в $h_k$ с некоторым затуханием $\\alpha$ мы накапливаем все пердыдущие градиенты, а после уже шагаем в сторону усреднённого градиента."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdd3b12",
   "metadata": {},
   "source": [
    "### AdaGrad и RMSprop\n",
    "\n",
    "**Вторая проблема** (Для стохастического градиента)\n",
    "\n",
    "Пусть имеется разреженный признак $x_j$ - почти на всех объектах нулевой (One-hot encoding). В линейной модели данный признак будет иметь следующий вид:\n",
    "\n",
    "$\\large a(x) = \\ldots + w_j x_j + \\ldots$\n",
    "\n",
    "Если $x_j = 0$, тогда частная производная по $w_j$ тоже будет равна нулю:\n",
    "\n",
    "$\\large x_j = 0 \\Longrightarrow \\frac{\\partial Q}{\\partial w_j} = 0$ А это значит, что на большенстве шагов $w_j$ не будет изменяться. И проблема заключается в том, что $\\eta_k$ убывает по мере роста $k$. На начальных итерациях $\\eta_k$ имеет большое значение, так как мы можем $1000$ итераций не изменять вес $w_j$, а $\\eta_k$ будет убывать, то когда попадается не нулевой признак $w_j$, но длина шаг может быть маленькой, так как мы прошли $1000$ итераций и не разу не изменили $w_j$.\n",
    "\n",
    "Если признаки редко принимают не нулевые значения, тогда необходимо для таких признаков изменять длину шага другим образом.\n",
    "\n",
    "### AdaGrad\n",
    "\n",
    "методе AdaGrad предлагается сделать свою длину шага для каждой компоненты вектора параметров.При этом шаг будет тем меньше, чем более длинные шаги мы делали на предыдущих итерациях:\n",
    "\n",
    "$\\large G_{kj} = G_{k-1,j} + (\\nabla_w Q(w^{(k-1)}))_j^2$ Где $k$ - номер итерации. $j$ - номер признака.\n",
    "\n",
    "$\\large w_j^{(k)} = w_j^{(k-1)} - \\frac{\\eta_k}{\\sqrt{G_{kj} + \\epsilon}} (\\nabla_w Q(w^{(k-1)}))_j$\n",
    "\n",
    "$G_kj$ - Накапливается информация о том, насколько большой шаг по $w_j$ \n",
    "\n",
    "### RMSprop\n",
    "\n",
    "У метода AdaGrad есть большой недостаток: переменная $G_{kj}$ монотонно растёт,\n",
    "из-за чего шаги становятся всё медленнее и могут остановиться ещё до того,\n",
    "как достигнут минимум функционала.\n",
    "Проблема решается в методе RMSprop, где используется экспоненциальное затухание градиентов:\n",
    "\n",
    "$\\large G_{kj} = \\alpha G_{k-1,j} + (1 - \\alpha) (\\nabla_w Q(w^{(k-1)}))_j^2$\n",
    "\n",
    "В этом случае размер шага по координате зависит в основном от того, насколько\n",
    "быстро мы двигались по ней на последних итерациях.\n",
    "\n",
    "### Adam - (Написать) сомешает два метода выше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dca62ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
