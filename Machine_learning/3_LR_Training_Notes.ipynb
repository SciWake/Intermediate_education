{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fb427cd",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#008B8B\">3. Обучение линейной регрессии</h1>\n",
    "\n",
    "Заметим, что здесь мы переписали выражение функционала ошибки, заменив $a(x)$ на скалярное призведение $\\left \\langle w,x \\right \\rangle$, после чего мы уже имеем функцию, а не функционал ошибки, так как $Q$ зависит не от некоторой функции $a(x)$, а от вектора весов $w$, и оптимизировать нужно именно по нему, что гораздо проще. Другими словами, в функционал ошибки необходимо подставить только веса, после чего произвести скалярное произведение весов на $x_i$ объект и получить предсказание модели.\n",
    "\n",
    "$\\large Q(w, X) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} (\\langle w, x_i \\rangle - y_i)^2 \\to \\underset{w}{\\text{min}}$\n",
    "\n",
    "Запишем эту задачу в матричном виде:\n",
    "\n",
    "$\\large X = \\begin{pmatrix}\n",
    "  x_{11} & x_{12} & \\ldots & x_{1d}\\\\\n",
    "  \\ldots & \\ldots & \\ldots & \\ldots\\\\\n",
    "  x_{\\ell 1} & x_{\\ell 2} & \\ldots & x_{\\ell d} \n",
    "\\end{pmatrix}$ - Матрица объекты-признаки\n",
    "\n",
    "$\\large y = \\begin{pmatrix}\n",
    "  y_{1}\\\\\n",
    "  \\ldots\\\\\n",
    "  y_{\\ell} \n",
    "\\end{pmatrix}$\n",
    "$\\large w = \\begin{pmatrix}\n",
    "  w_{1}\\\\\n",
    "  \\ldots\\\\\n",
    "  w_{d} \n",
    "\\end{pmatrix}$\n",
    "\n",
    "Теперь можно записать $\\text{MSE}$ в матричном виде:\n",
    "\n",
    "$\\large Q(w, X) = \\frac{1}{\\ell} \\|Xw - y\\|_2^2 \\to \\underset{w}{\\text{min}}$\n",
    "\n",
    "\n",
    "$\\large X_w = \\begin{pmatrix}\n",
    "  \\langle w, x_1 \\rangle\\\\\n",
    "  \\ldots\\\\\n",
    "  \\langle w, x_{\\ell} \\rangle \n",
    "\\end{pmatrix}$\n",
    "\n",
    "Найдём градиент:\n",
    "\n",
    "$\\large \\nabla_{w}Q(w,X) = \\frac{2}{\\ell}X^{T}(Xw-y)$\n",
    "\n",
    "Приравняем выражение градиента к $0$ для получения аналитического решения:\n",
    "\n",
    "$\\large \\frac{2}{\\ell}X^{T}(Xw-y) = 0$\n",
    "\n",
    "$\\large X^{T}(Xw-y) = 0$\n",
    "\n",
    "$\\large X^{T}Xw-X^{T}y = 0$\n",
    "\n",
    "$\\large w = \\frac{X^{T}y}{X^{T}X}$\n",
    "\n",
    "$\\large w^* = (X^T X)^{-1} X^T y$ - (только если $X$ полного ранга)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b17c37b",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#008B8B\">4. Градиентный спуск и оценивание градиента</h1>\n",
    "\n",
    "<h2 style=\"color:#008B8B\">4.1 Градиентный спуск</h2>\n",
    "\n",
    "**1)** $\\large w^{(0)}$ - начальное приближение\n",
    "\n",
    "Дальше необходимо сместиться в сторону, куда функция убывает быстрее всего, для этого необходимо найти антиградиент функции в точке с начальным приближением и двигаться в сторону наискорейшего убывания функции, а после произвести замену весов.\n",
    "\n",
    "$\\large \\nabla_w Q(w^{(k-1)}, X)$ - значение градиента функции потерь в точке $w^{(k-1)}$\n",
    "\n",
    "**2)** $\\large w^{(k)} = w^{k-1} - \\eta_k \\nabla_w Q(w^{(k-1)}, X)$ - шаг градиентного спуска\n",
    "\n",
    "\n",
    "**3) Процесс остановки**\n",
    "\n",
    "1. $\\large \\|w^{(k)} - w^{(k-1)} \\| < \\epsilon$ - Если на $k$ шаге вектор весов мало отличается от вектора весов на $k-1$ шаге, тогда производится остановка.\n",
    "\n",
    "2. $\\large \\|Q(w^{(k)}, X) - Q(w^{(k-1)}, X)\\| < \\epsilon$ - Если ошибка на обучении не меняется с предыдущим шагом\n",
    "\n",
    "\n",
    "**4) Сходимость**\n",
    "\n",
    "$\\large \\nabla_w Q(w, X) \\approx 0$ - Гарантируется, что при некоторых условиях градиентый спуск сходится к точке где градиент равен нулю (Это может быть минимум, максимум или седловая точка). \n",
    "\n",
    "Почти все функционалы ошибок для линейных моделей строго выпуклые, значит у данных функционалов одна точка и это точка глобального минимума. Для линейных моделей с матрицей полного ранга точка минимума будет одна и она будет являеться глобальной.\n",
    "\n",
    "Если решений несколько, тогда генерируем несколько начальных приближений и выбираем наименьшюу точку. Данный метод позволяет найти глобальный минимум, если имеются локальные минимумы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a391af4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
