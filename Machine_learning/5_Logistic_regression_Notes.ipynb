{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:black\" align=\"center\">Линейная классификация</h1>\n",
    "\n",
    "Мы помним что функционал ошибки выглядел следующим образом:\n",
    "\n",
    "$\\large Q(a, X) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} [a(x_i) = y_i]$ - долей правильных ответов (accuracy)\n",
    "\n",
    "Нам будет удобнее решать задачу минимизации, поэтому будем вместо этого использовать долю неправильных ответов:\n",
    "\n",
    "$\\large Q(a, X) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} [a(x_i) \\ne y_i]$ - доля ошибок.\n",
    "\n",
    "Подставили\n",
    "Так как этот функционал не дифференцируемый, то его необходимо заменить на функцию, которая оценивает сверху идикатор ошибки. Записали оценку сверху мы следующим образом:\n",
    "\n",
    "$\\large 0 \\le \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} [y_i \\langle w, x \\rangle < 0] \n",
    "\\le \n",
    " \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} \\tilde L (y_i \\langle w, x \\rangle) \\to \\underset{w}{\\text{min}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#008B8B\">1. Логистическая регрессия</h1>\n",
    "\n",
    "<h2 style=\"color:#008B8B\">1.1 Оценивание вероятностей</h2>\n",
    "\n",
    "### Для чего необходимо оценивать вероятность?\n",
    "\n",
    "Предположим, имеется модель $\\text{b(x)} \\in [0, 1]$. Представим, что модель выдала на некотором объекте уверенность равную $0.8$ в классе +1. Как это можно интерпретировать?\n",
    "\n",
    "Например, имеется множество из 10 объектов, где модель возвращает приблизительно одну веренность в своём прогнозе, тогда мы ожидаем, что среди этих объектов 80% будут положительными.\n",
    "\n",
    "Предположим, мы построили модель, кторая оценивает вероятность положительного класса. Записывать это будем следующим образом:\n",
    "\n",
    "$\\large b(x) \\approx p(y = +1 | x)$ - модель оценивает то, что класс положительный для данного объекта $x$. Оценивание веротяности принадлежности объекта к некоторому классу позволяет посмотреть уверенность модели в своём прогнозе. Также бывают задачи, когда нет понимания того, какой ответ в задачи: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример 1\n",
    "\n",
    "Пример, мы решаем задачу предсказания кликов пользователя по рекламе, где необходимо предсказать кликнет ли пользователь по данному рекламному банеру. Имеются следующие 3 категориальных признака по которым мы предсказываем нажмёт ли пользователь на рекламный банер.\n",
    "\n",
    "Так как объект описывается тремя идентификаторами, то может быть ситуация, когда пользователь отправлят один и тот же запрос на который показывается один и тот же банер:\n",
    "\n",
    "|id пользователя | id запроса | id банера | Нажал ли пользователь? |\n",
    "|:--------------:|:----------:|:---------:|:----------------------:|\n",
    "|10              |50          |527        |0                       |\n",
    "|10              |50          |527        |0                       |\n",
    "|10              |50          |527        |0                       |\n",
    "|10              |50          |527        |0                       |\n",
    "|10              |50          |527        |1                       |\n",
    "\n",
    "Пользователь может в большинстве случаев не нажимать на рекламу, спустся определённое количество запросов он кликнул на данный банер. Проблема заключается в том что один объект встречается много раз с рзными целевыми переменным (большенство раз 0). Если данную обучающую выборку передать классификатору, который возвращает только класс объекта, тогда ему будет тяжело извлечь закономерность из этой выборки, так как объект один, а ответы целевой переменной разыне. Тогда, можно сделать так, чтобы классификатор возвращал не класс, а вероятность пренадлежность объекта к некоторому классу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример 2\n",
    "\n",
    "Представим, что пользователь вводит некоторый запрос, на который необходим показать один из 4 банера. За каждый кликнутый банер мы получаем определённую сумму денег, обазначим как $c(x)$. Показывать самый дорогой банер будет плохой идей, так как пользователь может на него не нажать, для этого необходимо ввести вероятность клика $p(y = +1| x)$ которую оценивает модель.\n",
    "\n",
    "Что мы максимизируем в данной задаче? Необходимо максимизировать математическое ожидание прибыли, а что такое мат.ожидание прибыли? Это $с(x) \\cdot p(y = +1| x)$\n",
    "\n",
    "|$$c(x)$$|$$p(y = +1| x)$$ |$$с(x) \\cdot p(y = +1| x)$$ |\n",
    "|:------:|:---------------:|:--------------------------:|\n",
    "|1       |0.5              |0.5                         |\n",
    "|5       |0.1              |0.5                         |\n",
    "|10      |0.3              |3                           |\n",
    "|100     |0.01             |1                           |\n",
    "\n",
    "Лучшее математическое ожидание будет $3$. Значит данный банер лучше всего показывать. Соответсвенно, данная вероятность помогает выбрать то, что необходимо показывтаь пользователю."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример 3\n",
    "\n",
    "Банк принимает решение того, какой продукт необходимо предложить пользователю. Имеется продукт мобильный банк, который он подключит с вероятностью 0.9, либо мы можем предложить пользоватлею кредит, на который он согласится с вероятностью 0.1. Если предлогать пользователю продук по вероятности, тогда мы не сможем узнать цену рекомендации. Если на основе вероятности оценить то, сколько мы заработаем, тогда предложить крдеит пользвателю будет более хорошей идеей, так как прибыль намного будет больше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что такое оценивание вероятностей?\n",
    "\n",
    "Предположим, имется набор объектов $\\{x_1, \\ldots, x_n\\}$, который устроент так, что модель вовращает одинаковы числа на этих объетках $b(x_1) = b(x_2) = \\ldots = b(x_n)$, а целивая переменная на данных объектах имеет значения $\\{y_1, \\ldots, y_n\\}$.\n",
    "\n",
    "Мы хотим, чтобы модель выдала на этих объектах долю положительных среди них. Это требование можно записать через функцию потерь:\n",
    "\n",
    "$\\large \\underset{b \\in \\mathbb{R}}{\\text{argmin}} \\frac{1}{n} \\sum\\limits_{i=1}^n L(y_i, b)$ - ищем такое $b$ из всех возможных, которое даёт минимальную среднюю ошибку на $n$ объектах.\n",
    "\n",
    "И мы хотим, чтобы данная константа совпадала с долей положительных объектов:\n",
    "\n",
    "$\\large \\underset{b \\in \\mathbb{R}}{\\text{argmin}} \\frac{1}{n} \\sum\\limits_{i=1}^n L(y_i, b) \\approx \\frac{1}{n} \\sum\\limits_{i=1}^n [y_i = +1]$\n",
    "\n",
    "Стоит подметить, что модель выдаёт один и тот же прогноз на множестве $\\{x_1, \\ldots, x_n\\}$ объектов, мощность которого $n$ (это не все возможные объекты).\n",
    "\n",
    "Запишем более точную формулировку, так как мы работаем с вероятностью. Для этого будем считать, что $n \\to \\infty$, другими словами, имеется некоторая область (множество) из всех возможных объектов, где бесконечное количество объектов и модель на них возвращает одинаковый прогноз. Тогда посчитаем математическое ожидание функции потерь на объекте $x$ и мы требуем, чтобы $\\text{argmin}$ данного математического ожидания достигался на вероятности положительного класса для данного объетка $p(y = +1 | x)$\n",
    "\n",
    "$\\large \\underset{b \\in \\mathbb{R}}{\\text{argmin}} =\\mathbb{E} (L(y, b)|x) = p(y = +1 | x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Переход от первой формулы к второй**\n",
    "\n",
    "В первой формуле предпологается, что есть конечное множество объектов, которых $n$ штук. Функция потерь устроена так, что если мы будем искать оптимальную константу $b$ для прогноза на $n$ объектах, тогда оптимальное значение $b$ должно быть примерно равно доле положительных объектов в этом множестве. Значит, мы требуем для множества объектво где модель выдаёт один и тот же прогноз, чтобы этот прогноз был приерно равен вероятности положительного класса.\n",
    "\n",
    "Переход к второй формуле. Мы говорим, что $n$ теперь стремится к бесконечности. Имеется один объект $x$, экземпляров которого большое количество, а именно $n$ раз данный объект встречается в выборке и на этом объекте разные значения целевой переменной. Тогда, мы считаем математическое ожидание функции потреть по распределению $y$ в данной точке и минимальное математическое ожидание должно достигаться при константе $b$ равной вероятности положительного класса.\n",
    "\n",
    "$b$ оценивает вероятность положительного класса, поэтому $b$ должно аппроксимировать веротяность положительного класса. \n",
    "\n",
    "(не уверне в этом предложении) Почему у нас есть $n$ объектов, где прогноз одинаковый? Так как это один объект, значит и прогноз и целевая переменная будут иметь одно значение. Такую ситуацию мы уже разобрали выше, поэтому формула один была переписана на вероятностный язык."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Про математическое ожидание**\n",
    "\n",
    "Математическое ожидание берётся по распределению $y$ при условии данного $x$ $p(y|x), где $$y \\in \\{-1, +1\\}$. Математическое ожидание будет иметь следующий смысл:\n",
    "\n",
    "Это вероятность того, что $y$ равен +1 при условии $x$...\n",
    "\n",
    "$\\large \\mathbb{E} = p(y=+1 |x) \\cdot L(+1, b) + p(y = -1 | x) \\cdot L(-1, b)$\n",
    "\n",
    "\n",
    "Почему записали математическое ожидание? Так как мы $n$ устремляем в бесконечность, тогда вместо средней ошибки можно записать математическое ожидание ошибки.  А дальше, по закнону больших чисел в первой формуле, доля положительных объектов будет стремится к вероятности положительного класса. Ну и мы дальше во второй формуле требуем строгое равенство, чтобы функция потерь была устроена так, что для точки с бесконечным количеством объектов оптимально выдавать прогноз равный вероятности положитлеьного класса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Дополнительные вопросы**\n",
    "\n",
    "Что означает запись при условии $x$? Это означает, что мы рассматриваем распределение ответов конкретно для данного объекта. Допусим, имется объект $x_1$ на котором вероятность положитлеьного класса $p(y = +1 |x) = 0.9$, есть другой объект $x_2$, где вероятность равна $p(y = +1 |x) = 0.4$. Получается, что это вероятность того, что данный объект $x$ положительный.\n",
    "\n",
    "Что означает распределение ответов для одного объекта? Мы значем что объект один, но этот объект может встречаться несколько раз, это мы рассаматривали выше. Предположи, есть пространство объектов, где много положителных и среди них в середине расположен один отрицательный, тогда вероятнотность того, что этот объект будет отрицательным в данной области (множестве) будет небольшой. Поскольку эти объекты расположены рядом и модель вовзращает практически идентичный прогноз, тогда мы ожидаем, что вероятность того, что этот объект отрицательный будет небольшой. \n",
    "\n",
    "Под объекта расположенными рядом понимается то, что объекты с практически одинаковым признаковым описанием, которые группируются в признаковом простарнстве (мы это называли множеством), тогда логично,что скорее всего они относятся к одной группе. Соответсвенно записи формул по сумме $n$ объектов понимается что, имеется некая группа мощностью $n$, где их признаковое описание практически похоже, значит и модель будет выдавать приблизительно один резултат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример 4\n",
    "\n",
    "Применим требование для квадратичной ошибки.\n",
    "\n",
    "$\\large L(y, b) = (y-b)^2$\n",
    "\n",
    "Посмотрим, что будет оптимальное для такой ошибки, если у нас имеется много объектов с одинаковым признаковым описанием:\n",
    "\n",
    "$\\large \\mathbb{E} ((y - b)^2 | x) = p(y = +1 |x) \\cdot (1 - b)^2 + p(y = 0|x) \\cdot (-b)^2$\n",
    "\n",
    "Для того, чтобы требование выполнялось для $MSE$, необходимо пронумировать классы $y \\in \\{0, 1\\}$. \n",
    "\n",
    "$p(y = +1 |x) \\cdot (1 - b)^2$ - вероятность того, что класс положительный;\n",
    "\n",
    "$p(y = 0|x) \\cdot (-b)^2$ - вероятность того, что класс отрицательный.\n",
    "\n",
    "Найдём минимум математического ожидания:\n",
    "\n",
    "$\\large \\frac{\\partial}{\\partial b} = 2 \\cdot p(y = +1 |x) (b-1) + 2p(y=0|x)b = $\n",
    "\n",
    "Заметим, что $p(y=0|x) = (1-p(y = +1|x))$. Для краткости заменим $p(y = +1|x) = p$.\n",
    "\n",
    "$\\large = 2pb -2p + 2b - 2pb = 0$\n",
    "\n",
    "Получаем, что $\\large b = p(y = +1|x)$ - оптимально для модели (константы) быть равной вероятности положительного класса. \n",
    "\n",
    "**Как это можно интерпретировать?** Что такое вероятность положительного класса для данного объетка? Этооля положительных экземпляров среди объектов, если экземпляров данного объетка много. После эту долю мы умнжаем на ошибку в случае, если прогноз $b$ на данный экземплярах. Дальше долю отрицательных положительных экземпляров умножаем на ошибку при прогнозе $b$ для отрицатлеьных объектов и смотрим оптимальное значение $b$, оказыватся что опитмально брать $b$ в качестве вероятности положительного класса. Получается, если мы рассматриваем среднеквадратичную ошибку, тогда она устроена так, что для ней оптимально выдавать в некоторой области долю положитлеьных объектов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример 5\n",
    "\n",
    "Если првоерить данное свойство для абсолютной ошибки:\n",
    "\n",
    "$\\large L(y, b) = |y - b|$\n",
    "\n",
    "Опитмальное $b\\in \\{0, 1\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что мы рассмотрели выше?\n",
    "Пусть выполнено следующее свойство:\n",
    "\n",
    "$\\large \\underset{b \\in \\mathbb{R}}{\\text{argmin}} =\\mathbb{E} (L(y, b)|x) = p(y = +1 | x)$\n",
    "\n",
    "1. Если один объект $x$ встречается много раз в выборке и нет других объектов  в выборке, тогда модель $b(x)$ будет оценивать вероятность положительного класса. (такуой случай не встречается на практике, он теоретический)\n",
    "\n",
    "2. Если много разных объектов в выборке и нет ограничений на прогноз в каждом из них (модель может быть любой функцией, где на каждом объекте она может вернуть любое чилсо), тогда нет гарантий что данное свойство будет выполняться. \n",
    "\n",
    "3. Если много разных объектов в выборке и прогозны по объектам ограничиваются видном модели. Это означает, например, если некоторая метрика расстояния между двумя объектами небольшая, тогда выходы объектов примерно равны на этих двух объектах:\n",
    "\n",
    "    $\\large p(x_1, x_2) < \\epsilon \\Longrightarrow b(x_1) \\approx b(x_2)$\n",
    "    \n",
    "    Из-за того, что выходы модели связаны на объекта, тогда свойство будет выдавать приблизительно тоже самое. Хотя точно похожих объектов нет, но есть объекты, на которых выходы можеди похожи, тогда изходя из свойства, мы будем примено оценивать их вероятность."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функционал для квадрата ошибки\n",
    "\n",
    "Мы проверили, что квадарт ошибки $\\large L(y - b)^2$ обладает нужным свойством, поэтому его можно использовать для обучения классификатора который оцениват уверенность. Толко $b \\in \\{0, 1\\}$, если взять $b(x) = \\langle w, x \\rangle$ равное скалярному произведению, тогда не сходистя, ведь скалярное произведение возвращает любое вещественное число, для решения это проблемы можно придумать преобразование скалярного приозведения чтобы оно переводило вещественно число в отрезок от 0 до 1. Мы будем использовать для этого сигмойду:\n",
    "\n",
    "$\\large \\sigma (z) = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "Тогда обучать линейный классификатор будем на следующем функционале:\n",
    "\n",
    "$\\large \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} (\\sigma (\\langle w, x_i \\rangle) - y_i)^2 \\to \\underset{w}{\\text{min}}$\n",
    "\n",
    "Вот мы записали модель, которая будет оценивать верояность.\n",
    "\n",
    "**Чем плохой этот функционал?**\n",
    "\n",
    "Сигмойда быстро достигает 0 или 1 и становится практически константой. Предположим что на объекте  $x_1$, где целевая переменная равана $y=1$ веса имеют такое значение, что их скалярное произведение даст $\\langle w, x_1 \\rangle = -1000$, тогда $\\sigma(\\langle w, x_1 \\rangle) \\approx 0$. Ну и видим, что ошибка получается огромной.\n",
    "\n",
    "Допустим мы используем стохастический градиентый спуск и считаем градинт функционала по $w$:\n",
    "\n",
    "$\\large \\nabla_w (\\sigma(\\langle w, x_1 \\rangle) - y_1)^2= (\\sigma(\\langle w, x_1 \\rangle) - y_1)^2 \\cdot \\sigma'(\\langle w, x_1 \\rangle) \\cdot x_1$\n",
    "\n",
    "Здесь мы видим производную сигмойды $\\sigma'(\\langle w, x_1 \\rangle)$ в точке примеро равной $-1000$, а чему будет равна производна в этой точке если посмотреть на график? Конечно же примерно нулую, тогда и градиент тоже будет примерно равен нулю.\n",
    "\n",
    "Получается, если мы ошибаемся на объекте и попадаем константную зону сигмойды, тогда градиентый спуск не сможет оттуда выйти, это называется затуханием градиента. Поэтому использовать $MSE$ для обучения такой модели будет плохой идеей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#008B8B\">1.2 Правдоподобие и логистические потери</h2>\n",
    "\n",
    "Имеются объекты $x_1, \\ldots, x_{\\ell}$ с правильными ответами $y_1, \\ldots, y_{\\ell}$, где $y \\in \\{-1, +1\\}$. А модель оценивает веротяность положительного класса $b(x_1), \\ldots, b(x_{\\ell})$. Для каждой точки известна веротяность того, что этот объект положительный (с точки зрения модели). Тогда запишем правдоподобие, которое говорит о том, насколько модель согласуется с данными:\n",
    "\n",
    "$\\large Q(a, X) = \\prod\\limits_{i = 1}^{\\ell} b(x_i)^{[y_i = +1]} (1 - b(x_i))^{[y_i = -1]} \\to \\underset{w}{\\text{max}}$\n",
    "\n",
    "Мы проходимся по всем объектам, если объект положителен, тогда максимизируем $b(x_i)^{[y_i = +1]}$, то есть вероятность того, что это положительный класс должна быть близка к еденице, так как целевая переменная имеет значение 1. Если объект отрицателен, тогда вероятность положительного класса на этом обоъекте должна быть близка к нулю $b(x_i))^{[y_i = -1]}$.\n",
    "\n",
    "Правдоподобие будет максимальным, если для каждого положитлеьного объекта $b(x) = 1$, а для каждого отрицательного $b(x) = 0$.\n",
    "\n",
    "Данное правдоподобие можно использовать как функционал для обучения алгоритма, с той лишь оговоркой, что удобнее оптимизировать его логарифм, который позволяет убрать произведение чисел от 0 до 1, а миниус нужен для замены максимизации на минимизацию:\n",
    "\n",
    "$\\large -\\sum\\limits_{i = 1}^{\\ell} \\left( [y_i = +1] \\log b(x_i) + [y_i = -1] \\log (1 - b(x_i)) \\right) \\to \\underset{w}{\\text{min}}$\n",
    "\n",
    "Функция потреь записывается следующим образом:\n",
    "\n",
    "$\\large L(y, b) = -[y_i = +1] \\log b - [y = -1] \\log (1-b)$ - log-loss\n",
    "\n",
    "Данная функция потерь называется логарифмической (log-loss). Покажем, что она также позволяет корректно предсказывать вероятности. Запишем матожидание функции потерь в точке $x$:\n",
    "\n",
    "$\\large \\mathbb{E} \\biggl[L(y, b) | x \\biggr] = \\mathbb{E} \\biggl[-[y = +1] \\log b - [y = -1] \\log(1 - b) | x \\biggr] =$\n",
    "$\\large -p(y = +1 | x) \\log b - (1 - p(y = +1 | x)) \\log(1 - b)$\n",
    "\n",
    "Продифференцируем по $b$:\n",
    "\n",
    "$\\large \\frac{\\partial}{\\partial b} \\mathbb{E} \\biggl[L(y, b) | x \\biggr] =\n",
    "    -\\frac{p(y = +1 | x)}{b} + \\frac{1 - p(y = +1 | x)}{1 - b} = 0 \\Longrightarrow  b = p(y = +1|x)$\n",
    "\n",
    "Получаем, что $\\large b = p(y = +1|x)$ - оптимально для модели (константы) быть равной вероятности положительного класса. И эту функцию потреть можно уже использовать для оптимизации модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#008B8B\">1.3 Логистическая регрессия</h2>\n",
    "\n",
    "Тогда $\\large b(x) = \\sigma(\\langle w, x \\rangle)$ и подставим это $b(x)$ в log-loss:\n",
    "\n",
    "$\\large -\\sum\\limits_{i = 1}^{\\ell} \\left([y_i = +1] \\log \\frac{1}{1 + \\exp(-\\langle w, x_i \\rangle)} + [y_i = -1] \\log \\frac{\\exp(-\\langle w, x_i \\rangle)}{1 + \\exp(-\\langle w, x_i \\rangle)} \\right) =$\n",
    "\n",
    "Разделим числитель и знаменатель на числитель, после перевернём дробь:\n",
    "\n",
    "$\\large \\log \\frac{\\exp(-\\langle w, x_i \\rangle)}{1 + \\exp(-\\langle w, x_i \\rangle)} = \\log \\frac{1}{1 + \\exp(\\langle w, x_i \\rangle)}= \\log 1 + \\exp(\\langle w, x_i \\rangle)$ \n",
    "\n",
    "Логарифм дроби это минус логарифм перевёрнутой дроби, тогда внесём минус внутрь:\n",
    "\n",
    "$\\large = \\sum\\limits_{i = 1}^{\\ell} \\biggl([y_i = +1] \\log (1 + \\exp(-\\langle w, x_i \\rangle)) + [y_i = -1] \\log (1 + \\exp(\\langle w, x_i \\rangle) \\biggl)=$\n",
    "\n",
    "Можем заметить, что эти два слогаемых одинаковые, только под экспонентой одного стоит минус, а другого плюс. Мы можем убрать идикатор и записать всё это как единое выражение внеся вутрь экспоненты $-y_i$. Тогда если $y_i$ положителен, то получаем логариф от экспоненты... минус скалряное произведение, как в первом слагаемом, а если $y_i$ отрицательный, тогда то получаем логариф от экспоненты... плюс скалярное произведение как во втором слогаемом. Значит, это две идентичные записи:\n",
    "\n",
    "$\\large = \\sum\\limits_{i = 1}^{\\ell} \\log \\left(1 + \\exp(-y_i \\langle w, x_i \\rangle) \\right)$\n",
    "\n",
    "И эту функцию потерь мы записывали, которую мы называли логистической. Здесь есть отступ. У этой функции все нормально с градиентом. Этот метод называется логистической регрессией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h1 style=\"color:#008B8B\">2. Метод опорных векторов</h1>\n",
    "\n",
    "<h2 style=\"color:#008B8B\">1.1 Оценивание вероятностей</h2>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
