{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:black\" align=\"center\">Линейная регрессия</h1>\n",
    "\n",
    "<h2 style=\"color:#008B8B\">1. Регуляризация</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large Q(w, X)$ - Функционал ошибки\n",
    "\n",
    "Заметим, что здесь мы переписали выражение функционала ошибки, заменив $a(x)$ на скалярное призведение $\\left \\langle w,x \\right \\rangle$, после чего мы уже имеем функцию, а не функционал ошибки, так как $Q$ зависит не от некоторой функции $a(x)$, а от вектора весов $w$, и оптимизировать нужно именно по нему, что гораздо проще.\n",
    "\n",
    "$\\large Q(w, X) + \\alpha \\|w\\|_2$ - регуляризованный функционал, где \n",
    "\n",
    "$\\left \\| w \\right \\|$ - регуляризатор, а $\\alpha$ - коэффициент регуляризации, который должен быть равен или больше нуля. При этом, в регуляризатор не должен входить свободный коэффициент $w_0$, так как одна целевая переменная может иметь большой масштаб, а другая маленький и если регулиризовать $w_0$, тогда модель не сможет соответствовать этому масштабу (нельзя построить хорошую модель). Так же регуляризация может плохо работать если признаки не масштабированы.\n",
    "\n",
    "Дальше все это минимизируем по весам:\n",
    "\n",
    "$\\large Q(w, X) + \\alpha \\|w\\|_2 \\to \\underset{w}{\\text{min}}$\n",
    "\n",
    "**Аналитическое решение для случая с регуляризацией:**\n",
    "\n",
    "$\\large w = (X^T X + \\alpha I)^{-1} X^T y$ - (Для MSE)\n",
    "\n",
    "Благодаря добавлению диагональной матрицы к $X^T X$ данная матрица оказывается положительно определённой, и поэтому её можно обратить. Таким образом, при\n",
    "использовании $L_2$ регуляризации решение всегда будет единственным.\n",
    "\n",
    "Регуляризация позволяет ограничить большие веса модели, поэтому её стоит использовать. Главная проблема больших весов заключается в том, что при небольшом изменении, например, площади квартиры прогноз изменится на огромную сумму. Тем самым, модель с регуляризацией становится менее чувствительной к небольшим изменениям значения признаков.\n",
    "\n",
    "**$L_1$ регуляризация:**\n",
    "\n",
    "$\\large Q(w, X) + \\alpha \\|w\\|_1 \\to \\underset{w}{\\text{min}}$\n",
    "\n",
    "$L_1$ регуляризатор способен занулять некоторые веса, чем больше $\\alpha$, тем больше весов будут зануляться. Такое не происходит с $L_2$ регуляризатором.\n",
    "\n",
    "**Линейная зависимость признаков**\n",
    "\n",
    "Пусть в выборке есть линейно зависимые признаки. Это по определению означает, что существует такой вектор $v$, что для любого объекта $x$ выполнено $\\langle v, x \\rangle = 0$.\n",
    "\n",
    "Допустим, мы нашли оптимальный вектор весов $w$ для линейного классификатора.\n",
    "Но тогда классификаторы с векторами $w + \\alpha v$ будут давать точно такие же\n",
    "ответы на всех объектах, поскольку:\n",
    "\n",
    "$\\large \\langle w + \\alpha v, x \\rangle =\\langle w, x \\rangle + \\alpha \\underbrace{\\langle v, x \\rangle}_{=0} = \\langle w, x \\rangle$\n",
    "\n",
    "Модель с данным набором весов $w + \\alpha v$ даёт такие же прогнозы как у линейной модели с набором весов $w$. Так как две модели дают одинаковые прогнозы, значит $MSE$ у моделей равны, тогда у знадачи оптимизации решений много. \n",
    "\n",
    "Основная проблема заключается в том, что если взять веса $w + \\alpha v$ и $\\alpha$ устремить в бесконечность, то мы будем с ростом $\\alpha$ получать решения, где веса будут всё больше и больше. Данные решения дадут одинаковый результат предсказания на обучающей выборке, но на отложенной выборке мы получим переобучение, так как веса имеют огромные значения.\n",
    "\n",
    "Решением этой проблемы становится регуляризация. С точки зрения $MSE$ все эти наборы весов одинаковы, но с точки зрения $L_2$ нормы есть решения с наименьшими весами. Значит, регуляризованный $MSE$ после минимизации способен подобрать веса с наилучшей ошибкой и при этом, веса будут иметь небольшие значения по модулю."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#008B8B\">2. Разреженные модели</h2>\n",
    "\n",
    "$\\large Q(w, X) + \\alpha \\|w\\|_1 \\to \\underset{w}{\\text{min}}$\n",
    "\n",
    "У $L_1$ регуляризатора есть свойсво, что чем больше $\\alpha$, тем больше будет нулевых весов в итоговм векторе. Когда производится зануление части весов - это называется разреженной моделью. Зачем это делают?\n",
    "\n",
    "1. Увеличение скорости - (выбрасываем веса, которые зануляются);\n",
    "\n",
    "2. Убираем признаки, которые не имеют пользы;\n",
    "\n",
    "3. Решение проблемы, когда данных меньше признаков.\n",
    "\n",
    "**Объяснение 1. Почему $L_1$ регуляризатор отбирает признаки?**\n",
    "\n",
    "Решение данной задачи $\\large Q(w, X) + \\alpha \\|w\\|_1 \\to \\underset{w}{\\text{min}}$ эквивалетно решению условной задачи оптимизации:\n",
    "\n",
    "$\\left\\{\n",
    "    \\begin{aligned}\n",
    "        & Q(w) \\to \\min_{w} \\\\\n",
    "        & \\|w\\|_1 \\leq C\n",
    "    \\end{aligned}\n",
    "\\right.$\n",
    "\n",
    "Данные задачи эквиваленты, если правильно подобрать значение $C$ под $\\lambda$. Если построить линии уровня функционала качества, а так же ограничения, задаваемы $L_2$ и $L_1$ регуляризаций, тогда при условии выше, ограничения буду выглядеть как ромб и пересекаться скорее всего на верхушке ромба, тогда один признак на графике принимает значение равное 0. А если в задаче выше подставим $L_2$ регуляризацию:\n",
    "\n",
    "$\\left\\{\n",
    "    \\begin{aligned}\n",
    "        & Q(w) \\to \\min_{w} \\\\\n",
    "        & \\|w\\|_2 \\leq C\n",
    "    \\end{aligned}\n",
    "\\right.$\n",
    "\n",
    "Тогда ограничения будут иметь форму круга и пересечение произойдет скорее всего сбоку."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Объяснение 2. Почему $L_1$ регуляризатор отбирает признаки?**\n",
    "\n",
    "Имеется вектор весов $w = (1, \\epsilon)$. \n",
    "\n",
    "Уменьшим первую координату на $\\delta$, где $0 < \\delta < \\epsilon < 1$ и посмотрим как изенится норма:\n",
    "\n",
    "$\\large \\|w - (\\delta, 0)\\|_2^2 = (1 - \\delta)^2 + \\epsilon^2 = 1 - 2\\delta + \\delta^2 + \\epsilon^2$\n",
    "\n",
    "$\\large \\|w - (\\delta, 0)\\|_1  = 1 - \\delta + \\epsilon$\n",
    "\n",
    "Уменьшим вторую координату на $\\delta$, где $\\delta < \\epsilon$ и посмотрим как изенится норма:\n",
    "\n",
    "$\\large \\|w - (0, \\delta)\\|_2^2 = 1^2 + (\\epsilon - \\delta)^2 = 1 - 2\\epsilon \\delta + \\delta^2 + \\epsilon^2$\n",
    "\n",
    "$\\large \\|w - (0, \\delta)\\|_1  = 1 - \\delta + \\epsilon$\n",
    "\n",
    "С точки зрения $L_1$ нормы нет разницы какую координату уменьшаем. Значит и нет разницы какие по размеру веса мы изменяем, большие или маленькие. \n",
    "\n",
    "$L_2$ норма меньше в первом случае, чем во втором. Получается, что с точки зрения $L_2$ нормы выгоднее изменять большие веса, чем мальнькие, так в этом случае ошибка будет уменьшаться быстрее всего. Поэтому, если имеется маленький вес, то $L_2$ норма его не будет уменьшать, ей выгоднее уменьшить большие веса, нежели маленькие.\n",
    "\n",
    "А для $L_1$ нормы нет разницы что уменьшать, так как ошибка изменяется равнозначно как у больших так и у маленьких весов."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
